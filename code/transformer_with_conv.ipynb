{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mlZp9TiqQizo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preprocessing**\n",
        "\n"
      ],
      "metadata": {
        "id": "mlZp9TiqQizo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlEKd5C1QLM-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful Constants\n",
        "\n",
        "# Those are separate normalised input features for the neural network\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"x_accel\",\n",
        "    \"y_accel\",\n",
        "    \"z_accel\",\n",
        "]\n",
        "\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [\n",
        "    \"Walking\", \n",
        "    \"Jogging\", \n",
        "    \"Uptairs\", \n",
        "    \"Downstairs\", \n",
        "    \"Sitting\", \n",
        "    \"Standing\"\n",
        "] "
      ],
      "metadata": {
        "id": "JB-5knB0QnRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "x = genfromtxt('/content/drive/MyDrive/Colab Notebooks/WISDM_x.csv', delimiter=',')\n",
        "y = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/WISDM_y.csv').to_numpy()"
      ],
      "metadata": {
        "id": "u-50JJVOQnUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_series(x,y,timestep,overlap):\n",
        "\n",
        "  slide_step = int(timestep*(1-overlap))\n",
        "  data_num = int((len(x)/slide_step)-1)\n",
        "  \n",
        "  dataset = np.ndarray(shape=(data_num,timestep,len(x[0])))\n",
        "  labels = list()\n",
        "\n",
        "  for i in range(data_num):\n",
        "    labels.append(y[slide_step*(i+1)-1])\n",
        "    for j in range(timestep):\n",
        "      dataset[i,j,:] = x[slide_step*i+j,:]\n",
        "\n",
        "  return dataset,np.array(labels)\n"
      ],
      "metadata": {
        "id": "2yq5BuVsQncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "CvEWOmYec59V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6216500d-f180-439e-9b60-3243e4c89005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048574, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y).reshape(-1,1)\n",
        "\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "y = ohe.fit_transform(y).toarray()\n"
      ],
      "metadata": {
        "id": "2gGVFhIBQsiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4971c78a-6ee8-4573-e175-3fc97339ed57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred.append(\"5\")\n",
        "dataset,labels = create_series(x,y,64,0.5)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset,labels,test_size=0.3, random_state=412)\n",
        "# X_test, X_val, y_test, y_val = train_test_split(X_toSplit,y_toSplit,test_size=0.25, random_state=412)"
      ],
      "metadata": {
        "id": "EevSjWUpQslY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape\n",
        "# y_train[100]"
      ],
      "metadata": {
        "id": "JAJvdsoocM8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ccdf78-08c1-4a98-a9d5-36078c93530b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22936, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Data \n",
        "\n",
        "training_data_count = len(X_train)  # 96092 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 41183 testing series\n",
        "n_steps = len(X_train[0])  # 16 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 3 input parameters per timestep\n",
        "\n",
        "\n",
        "# LSTM Neural Network's internal structure\n",
        "# n_hidden = 32 # Hidden layer num of features\n",
        "n_classes = 6 # Total classes (should go up, or should go down)\n",
        "\n",
        "\n",
        "# Training \n",
        "\n",
        "learning_rate = 0.0025\n",
        "lambda_loss_amount = 0.0015\n",
        "training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
        "batch_size = 64\n",
        "display_iter = 30000  # To show test set accuracy during training\n",
        "\n",
        "\n",
        "# Some debugging info\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
      ],
      "metadata": {
        "id": "97hV-qp5QyL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffcd23c5-98a6-4e80-fd05-8909d776909d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(39321, 16, 3) (39321, 6) 2.873419266367973 6.908464983446615\n",
            "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "Ow0FThhqQ9LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "5CahU2v_RAbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "Ltmrm7PmSYaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "def getPositionEncoding(seq_len, d, n=10000):\n",
        "    P = np.zeros((seq_len, d))\n",
        "    for k in range(seq_len):\n",
        "        for i in np.arange(int(d/2)):\n",
        "            denominator = np.power(n, 2*i/d)\n",
        "            P[k, 2*i] = np.sin(k/denominator)\n",
        "            P[k, 2*i+1] = np.cos(k/denominator)\n",
        "    return P\n"
      ],
      "metadata": {
        "id": "DMWuP82CSYgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = getPositionEncoding(seq_len=16,d=3)\n",
        "c = X_train\n",
        "for i in range(X_train.shape[0]):\n",
        "  X_train[i] = X_train[i] + pe\n",
        "\n",
        "# print(c)\n",
        "# print('done')\n",
        "# print(X_train)\n"
      ],
      "metadata": {
        "id": "K5TcuoRt0GcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    #conv1D\n",
        "    x = layers.Conv1D(256, 3,input_shape=input_shape)(x)\n",
        "    # print(x.shape)\n",
        "    x = layers.Conv1D(128, 3)(x)\n",
        "    # print(x.shape)\n",
        "    x = layers.Conv1D(64, 3)(x)\n",
        "    # print(x.shape)\n",
        "    x = layers.Conv1D(32, 3)(x)\n",
        "    # print(x.shape)\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "lNid1lCySYdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.2,\n",
        ")\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "# model.summary()\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
        "\n",
        "model.fit(X_train,y_train,validation_split=0.1,epochs=200,batch_size=64, callbacks=[callback])\n",
        "\n",
        "model.evaluate(X_test, y_test, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALIZJr6ESYjE",
        "outputId": "2532f691-ec56-48ec-9578-d9b764b3ce3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1291/1291 [==============================] - 27s 18ms/step - loss: 0.7371 - accuracy: 0.7362 - val_loss: 0.5482 - val_accuracy: 0.8014\n",
            "Epoch 2/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.5388 - accuracy: 0.8051 - val_loss: 0.4470 - val_accuracy: 0.8265\n",
            "Epoch 3/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.4831 - accuracy: 0.8213 - val_loss: 0.4387 - val_accuracy: 0.8288\n",
            "Epoch 4/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.4455 - accuracy: 0.8332 - val_loss: 0.4429 - val_accuracy: 0.8335\n",
            "Epoch 5/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.4231 - accuracy: 0.8403 - val_loss: 0.4033 - val_accuracy: 0.8379\n",
            "Epoch 6/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.4059 - accuracy: 0.8453 - val_loss: 0.3782 - val_accuracy: 0.8449\n",
            "Epoch 7/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.3936 - accuracy: 0.8492 - val_loss: 0.3573 - val_accuracy: 0.8583\n",
            "Epoch 8/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.3823 - accuracy: 0.8532 - val_loss: 0.3551 - val_accuracy: 0.8524\n",
            "Epoch 9/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3711 - accuracy: 0.8557 - val_loss: 0.3456 - val_accuracy: 0.8587\n",
            "Epoch 10/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3688 - accuracy: 0.8565 - val_loss: 0.3436 - val_accuracy: 0.8611\n",
            "Epoch 11/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3593 - accuracy: 0.8610 - val_loss: 0.3441 - val_accuracy: 0.8642\n",
            "Epoch 12/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3508 - accuracy: 0.8632 - val_loss: 0.3419 - val_accuracy: 0.8580\n",
            "Epoch 13/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3463 - accuracy: 0.8646 - val_loss: 0.3361 - val_accuracy: 0.8695\n",
            "Epoch 14/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3420 - accuracy: 0.8667 - val_loss: 0.3482 - val_accuracy: 0.8699\n",
            "Epoch 15/200\n",
            "1291/1291 [==============================] - 24s 19ms/step - loss: 0.3383 - accuracy: 0.8679 - val_loss: 0.3258 - val_accuracy: 0.8669\n",
            "Epoch 16/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3327 - accuracy: 0.8716 - val_loss: 0.3320 - val_accuracy: 0.8724\n",
            "Epoch 17/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3297 - accuracy: 0.8715 - val_loss: 0.3257 - val_accuracy: 0.8755\n",
            "Epoch 18/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3249 - accuracy: 0.8733 - val_loss: 0.3527 - val_accuracy: 0.8669\n",
            "Epoch 19/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3255 - accuracy: 0.8723 - val_loss: 0.3398 - val_accuracy: 0.8620\n",
            "Epoch 20/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3188 - accuracy: 0.8759 - val_loss: 0.3188 - val_accuracy: 0.8783\n",
            "Epoch 21/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3163 - accuracy: 0.8771 - val_loss: 0.3154 - val_accuracy: 0.8778\n",
            "Epoch 22/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.3100 - accuracy: 0.8796 - val_loss: 0.3196 - val_accuracy: 0.8785\n",
            "Epoch 23/200\n",
            "1291/1291 [==============================] - 24s 19ms/step - loss: 0.3101 - accuracy: 0.8796 - val_loss: 0.3054 - val_accuracy: 0.8849\n",
            "Epoch 24/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3113 - accuracy: 0.8805 - val_loss: 0.3096 - val_accuracy: 0.8860\n",
            "Epoch 25/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3054 - accuracy: 0.8818 - val_loss: 0.3105 - val_accuracy: 0.8800\n",
            "Epoch 26/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.3062 - accuracy: 0.8818 - val_loss: 0.3093 - val_accuracy: 0.8835\n",
            "Epoch 27/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2995 - accuracy: 0.8855 - val_loss: 0.3130 - val_accuracy: 0.8904\n",
            "Epoch 28/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2995 - accuracy: 0.8849 - val_loss: 0.3107 - val_accuracy: 0.8832\n",
            "Epoch 29/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2965 - accuracy: 0.8852 - val_loss: 0.3101 - val_accuracy: 0.8919\n",
            "Epoch 30/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2916 - accuracy: 0.8893 - val_loss: 0.2998 - val_accuracy: 0.8856\n",
            "Epoch 31/200\n",
            "1291/1291 [==============================] - 25s 19ms/step - loss: 0.2893 - accuracy: 0.8890 - val_loss: 0.2925 - val_accuracy: 0.8924\n",
            "Epoch 32/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2926 - accuracy: 0.8872 - val_loss: 0.3048 - val_accuracy: 0.8897\n",
            "Epoch 33/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2893 - accuracy: 0.8897 - val_loss: 0.2817 - val_accuracy: 0.8922\n",
            "Epoch 34/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2870 - accuracy: 0.8901 - val_loss: 0.2985 - val_accuracy: 0.8848\n",
            "Epoch 35/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2853 - accuracy: 0.8914 - val_loss: 0.3028 - val_accuracy: 0.8911\n",
            "Epoch 36/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2829 - accuracy: 0.8924 - val_loss: 0.2979 - val_accuracy: 0.8925\n",
            "Epoch 37/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2852 - accuracy: 0.8912 - val_loss: 0.3002 - val_accuracy: 0.8900\n",
            "Epoch 38/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2786 - accuracy: 0.8936 - val_loss: 0.2981 - val_accuracy: 0.8889\n",
            "Epoch 39/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2802 - accuracy: 0.8937 - val_loss: 0.2846 - val_accuracy: 0.9014\n",
            "Epoch 40/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2789 - accuracy: 0.8922 - val_loss: 0.2965 - val_accuracy: 0.8956\n",
            "Epoch 41/200\n",
            "1291/1291 [==============================] - 24s 19ms/step - loss: 0.2747 - accuracy: 0.8945 - val_loss: 0.2981 - val_accuracy: 0.8933\n",
            "Epoch 42/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2755 - accuracy: 0.8958 - val_loss: 0.2847 - val_accuracy: 0.8970\n",
            "Epoch 43/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2740 - accuracy: 0.8963 - val_loss: 0.2887 - val_accuracy: 0.8945\n",
            "Epoch 44/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2688 - accuracy: 0.8976 - val_loss: 0.2853 - val_accuracy: 0.8984\n",
            "Epoch 45/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2720 - accuracy: 0.8975 - val_loss: 0.2961 - val_accuracy: 0.8962\n",
            "Epoch 46/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2712 - accuracy: 0.8974 - val_loss: 0.2867 - val_accuracy: 0.8965\n",
            "Epoch 47/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2724 - accuracy: 0.8966 - val_loss: 0.2788 - val_accuracy: 0.8947\n",
            "Epoch 48/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2657 - accuracy: 0.8994 - val_loss: 0.2909 - val_accuracy: 0.8946\n",
            "Epoch 49/200\n",
            "1291/1291 [==============================] - 24s 18ms/step - loss: 0.2659 - accuracy: 0.8987 - val_loss: 0.3037 - val_accuracy: 0.8936\n",
            "Epoch 50/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2679 - accuracy: 0.8982 - val_loss: 0.2749 - val_accuracy: 0.9017\n",
            "Epoch 51/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2634 - accuracy: 0.9020 - val_loss: 0.2830 - val_accuracy: 0.8940\n",
            "Epoch 52/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2652 - accuracy: 0.8996 - val_loss: 0.2774 - val_accuracy: 0.9038\n",
            "Epoch 53/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2661 - accuracy: 0.8999 - val_loss: 0.2781 - val_accuracy: 0.8995\n",
            "Epoch 54/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2618 - accuracy: 0.9011 - val_loss: 0.2810 - val_accuracy: 0.9047\n",
            "Epoch 55/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2642 - accuracy: 0.9006 - val_loss: 0.2650 - val_accuracy: 0.9063\n",
            "Epoch 56/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2571 - accuracy: 0.9040 - val_loss: 0.2702 - val_accuracy: 0.9050\n",
            "Epoch 57/200\n",
            "1291/1291 [==============================] - 25s 19ms/step - loss: 0.2599 - accuracy: 0.9025 - val_loss: 0.2739 - val_accuracy: 0.9068\n",
            "Epoch 58/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2575 - accuracy: 0.9024 - val_loss: 0.2906 - val_accuracy: 0.8971\n",
            "Epoch 59/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2591 - accuracy: 0.9025 - val_loss: 0.2756 - val_accuracy: 0.9044\n",
            "Epoch 60/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2546 - accuracy: 0.9041 - val_loss: 0.2692 - val_accuracy: 0.9034\n",
            "Epoch 61/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2557 - accuracy: 0.9031 - val_loss: 0.2656 - val_accuracy: 0.9058\n",
            "Epoch 62/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2518 - accuracy: 0.9049 - val_loss: 0.2691 - val_accuracy: 0.9088\n",
            "Epoch 63/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2533 - accuracy: 0.9052 - val_loss: 0.2862 - val_accuracy: 0.9020\n",
            "Epoch 64/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2517 - accuracy: 0.9067 - val_loss: 0.2731 - val_accuracy: 0.9025\n",
            "Epoch 65/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2552 - accuracy: 0.9047 - val_loss: 0.2870 - val_accuracy: 0.9008\n",
            "Epoch 66/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2486 - accuracy: 0.9075 - val_loss: 0.2805 - val_accuracy: 0.9005\n",
            "Epoch 67/200\n",
            "1291/1291 [==============================] - 26s 20ms/step - loss: 0.2490 - accuracy: 0.9075 - val_loss: 0.2670 - val_accuracy: 0.9094\n",
            "Epoch 68/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2496 - accuracy: 0.9071 - val_loss: 0.2635 - val_accuracy: 0.9063\n",
            "Epoch 69/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2500 - accuracy: 0.9076 - val_loss: 0.2772 - val_accuracy: 0.9006\n",
            "Epoch 70/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2459 - accuracy: 0.9085 - val_loss: 0.2758 - val_accuracy: 0.9075\n",
            "Epoch 71/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2469 - accuracy: 0.9085 - val_loss: 0.2647 - val_accuracy: 0.9108\n",
            "Epoch 72/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2444 - accuracy: 0.9088 - val_loss: 0.2749 - val_accuracy: 0.9025\n",
            "Epoch 73/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2460 - accuracy: 0.9093 - val_loss: 0.2730 - val_accuracy: 0.9016\n",
            "Epoch 74/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2459 - accuracy: 0.9096 - val_loss: 0.2665 - val_accuracy: 0.9069\n",
            "Epoch 75/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2426 - accuracy: 0.9098 - val_loss: 0.2849 - val_accuracy: 0.9035\n",
            "Epoch 76/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2400 - accuracy: 0.9101 - val_loss: 0.2763 - val_accuracy: 0.9035\n",
            "Epoch 77/200\n",
            "1291/1291 [==============================] - 25s 19ms/step - loss: 0.2402 - accuracy: 0.9113 - val_loss: 0.2733 - val_accuracy: 0.9101\n",
            "Epoch 78/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2389 - accuracy: 0.9110 - val_loss: 0.2918 - val_accuracy: 0.9068\n",
            "Epoch 79/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2422 - accuracy: 0.9103 - val_loss: 0.2900 - val_accuracy: 0.9021\n",
            "Epoch 80/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2385 - accuracy: 0.9110 - val_loss: 0.2707 - val_accuracy: 0.9107\n",
            "Epoch 81/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2379 - accuracy: 0.9119 - val_loss: 0.2665 - val_accuracy: 0.9132\n",
            "Epoch 82/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2415 - accuracy: 0.9114 - val_loss: 0.2799 - val_accuracy: 0.9072\n",
            "Epoch 83/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2381 - accuracy: 0.9117 - val_loss: 0.2702 - val_accuracy: 0.9086\n",
            "Epoch 84/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2403 - accuracy: 0.9114 - val_loss: 0.2723 - val_accuracy: 0.9094\n",
            "Epoch 85/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2355 - accuracy: 0.9129 - val_loss: 0.2743 - val_accuracy: 0.9090\n",
            "Epoch 86/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2389 - accuracy: 0.9123 - val_loss: 0.2763 - val_accuracy: 0.9082\n",
            "Epoch 87/200\n",
            "1291/1291 [==============================] - 25s 19ms/step - loss: 0.2359 - accuracy: 0.9130 - val_loss: 0.2724 - val_accuracy: 0.9076\n",
            "Epoch 88/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2347 - accuracy: 0.9141 - val_loss: 0.2709 - val_accuracy: 0.9071\n",
            "Epoch 89/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2337 - accuracy: 0.9137 - val_loss: 0.2740 - val_accuracy: 0.9124\n",
            "Epoch 90/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2352 - accuracy: 0.9131 - val_loss: 0.2702 - val_accuracy: 0.9093\n",
            "Epoch 91/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2339 - accuracy: 0.9147 - val_loss: 0.2661 - val_accuracy: 0.9082\n",
            "Epoch 92/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2321 - accuracy: 0.9149 - val_loss: 0.2740 - val_accuracy: 0.9107\n",
            "Epoch 93/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2321 - accuracy: 0.9165 - val_loss: 0.2694 - val_accuracy: 0.9112\n",
            "Epoch 94/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2308 - accuracy: 0.9159 - val_loss: 0.2779 - val_accuracy: 0.9066\n",
            "Epoch 95/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2290 - accuracy: 0.9152 - val_loss: 0.2750 - val_accuracy: 0.9135\n",
            "Epoch 96/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2288 - accuracy: 0.9161 - val_loss: 0.2744 - val_accuracy: 0.9135\n",
            "Epoch 97/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2310 - accuracy: 0.9156 - val_loss: 0.2710 - val_accuracy: 0.9113\n",
            "Epoch 98/200\n",
            "1291/1291 [==============================] - 25s 19ms/step - loss: 0.2287 - accuracy: 0.9162 - val_loss: 0.2725 - val_accuracy: 0.9054\n",
            "Epoch 99/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2296 - accuracy: 0.9161 - val_loss: 0.2804 - val_accuracy: 0.9112\n",
            "Epoch 100/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2287 - accuracy: 0.9166 - val_loss: 0.2856 - val_accuracy: 0.9113\n",
            "Epoch 101/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2291 - accuracy: 0.9164 - val_loss: 0.2772 - val_accuracy: 0.9140\n",
            "Epoch 102/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2275 - accuracy: 0.9177 - val_loss: 0.2703 - val_accuracy: 0.9116\n",
            "Epoch 103/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2267 - accuracy: 0.9182 - val_loss: 0.2722 - val_accuracy: 0.9115\n",
            "Epoch 104/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2265 - accuracy: 0.9181 - val_loss: 0.2708 - val_accuracy: 0.9140\n",
            "Epoch 105/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2276 - accuracy: 0.9184 - val_loss: 0.2708 - val_accuracy: 0.9113\n",
            "Epoch 106/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2269 - accuracy: 0.9173 - val_loss: 0.2755 - val_accuracy: 0.9101\n",
            "Epoch 107/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2287 - accuracy: 0.9178 - val_loss: 0.2751 - val_accuracy: 0.9112\n",
            "Epoch 108/200\n",
            "1291/1291 [==============================] - 25s 19ms/step - loss: 0.2260 - accuracy: 0.9183 - val_loss: 0.2674 - val_accuracy: 0.9124\n",
            "Epoch 109/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2211 - accuracy: 0.9198 - val_loss: 0.2757 - val_accuracy: 0.9125\n",
            "Epoch 110/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2271 - accuracy: 0.9169 - val_loss: 0.2687 - val_accuracy: 0.9119\n",
            "Epoch 111/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2240 - accuracy: 0.9181 - val_loss: 0.2961 - val_accuracy: 0.9084\n",
            "Epoch 112/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2203 - accuracy: 0.9187 - val_loss: 0.2582 - val_accuracy: 0.9107\n",
            "Epoch 113/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2235 - accuracy: 0.9193 - val_loss: 0.2636 - val_accuracy: 0.9143\n",
            "Epoch 114/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2232 - accuracy: 0.9183 - val_loss: 0.2683 - val_accuracy: 0.9147\n",
            "Epoch 115/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2215 - accuracy: 0.9188 - val_loss: 0.2698 - val_accuracy: 0.9119\n",
            "Epoch 116/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2230 - accuracy: 0.9187 - val_loss: 0.2672 - val_accuracy: 0.9160\n",
            "Epoch 117/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2231 - accuracy: 0.9192 - val_loss: 0.2725 - val_accuracy: 0.9132\n",
            "Epoch 118/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2200 - accuracy: 0.9206 - val_loss: 0.2660 - val_accuracy: 0.9150\n",
            "Epoch 119/200\n",
            "1291/1291 [==============================] - 25s 19ms/step - loss: 0.2212 - accuracy: 0.9188 - val_loss: 0.2700 - val_accuracy: 0.9127\n",
            "Epoch 120/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2210 - accuracy: 0.9204 - val_loss: 0.2604 - val_accuracy: 0.9169\n",
            "Epoch 121/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2193 - accuracy: 0.9195 - val_loss: 0.2658 - val_accuracy: 0.9143\n",
            "Epoch 122/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2221 - accuracy: 0.9191 - val_loss: 0.2661 - val_accuracy: 0.9131\n",
            "Epoch 123/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2169 - accuracy: 0.9227 - val_loss: 0.2844 - val_accuracy: 0.9139\n",
            "Epoch 124/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2164 - accuracy: 0.9211 - val_loss: 0.2764 - val_accuracy: 0.9155\n",
            "Epoch 125/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2199 - accuracy: 0.9212 - val_loss: 0.2649 - val_accuracy: 0.9195\n",
            "Epoch 126/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2164 - accuracy: 0.9221 - val_loss: 0.2697 - val_accuracy: 0.9155\n",
            "Epoch 127/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2202 - accuracy: 0.9203 - val_loss: 0.2672 - val_accuracy: 0.9186\n",
            "Epoch 128/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2148 - accuracy: 0.9225 - val_loss: 0.2624 - val_accuracy: 0.9132\n",
            "Epoch 129/200\n",
            "1291/1291 [==============================] - 26s 20ms/step - loss: 0.2165 - accuracy: 0.9227 - val_loss: 0.2676 - val_accuracy: 0.9155\n",
            "Epoch 130/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2177 - accuracy: 0.9211 - val_loss: 0.2625 - val_accuracy: 0.9171\n",
            "Epoch 131/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2158 - accuracy: 0.9220 - val_loss: 0.3001 - val_accuracy: 0.9113\n",
            "Epoch 132/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2163 - accuracy: 0.9214 - val_loss: 0.2682 - val_accuracy: 0.9163\n",
            "Epoch 133/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2143 - accuracy: 0.9230 - val_loss: 0.2666 - val_accuracy: 0.9161\n",
            "Epoch 134/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2189 - accuracy: 0.9218 - val_loss: 0.2869 - val_accuracy: 0.9117\n",
            "Epoch 135/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2141 - accuracy: 0.9232 - val_loss: 0.2795 - val_accuracy: 0.9157\n",
            "Epoch 136/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2153 - accuracy: 0.9230 - val_loss: 0.2799 - val_accuracy: 0.9180\n",
            "Epoch 137/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2138 - accuracy: 0.9229 - val_loss: 0.2742 - val_accuracy: 0.9132\n",
            "Epoch 138/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2140 - accuracy: 0.9234 - val_loss: 0.2768 - val_accuracy: 0.9183\n",
            "Epoch 139/200\n",
            "1291/1291 [==============================] - 25s 20ms/step - loss: 0.2087 - accuracy: 0.9259 - val_loss: 0.2713 - val_accuracy: 0.9163\n",
            "Epoch 140/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2154 - accuracy: 0.9222 - val_loss: 0.2727 - val_accuracy: 0.9169\n",
            "Epoch 141/200\n",
            "1291/1291 [==============================] - 23s 18ms/step - loss: 0.2140 - accuracy: 0.9234 - val_loss: 0.2830 - val_accuracy: 0.9174\n",
            "Epoch 142/200\n",
            "1291/1291 [==============================] - 24s 18ms/step - loss: 0.2133 - accuracy: 0.9234 - val_loss: 0.2653 - val_accuracy: 0.9227\n",
            "Epoch 143/200\n",
            "1291/1291 [==============================] - 23s 17ms/step - loss: 0.2119 - accuracy: 0.9244 - val_loss: 0.2618 - val_accuracy: 0.9181\n",
            "Epoch 144/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2153 - accuracy: 0.9234 - val_loss: 0.2777 - val_accuracy: 0.9165\n",
            "Epoch 145/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2142 - accuracy: 0.9232 - val_loss: 0.2621 - val_accuracy: 0.9203\n",
            "Epoch 146/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2119 - accuracy: 0.9246 - val_loss: 0.2631 - val_accuracy: 0.9179\n",
            "Epoch 147/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2109 - accuracy: 0.9255 - val_loss: 0.2723 - val_accuracy: 0.9173\n",
            "Epoch 148/200\n",
            "1291/1291 [==============================] - 22s 17ms/step - loss: 0.2116 - accuracy: 0.9243 - val_loss: 0.2744 - val_accuracy: 0.9150\n",
            "Epoch 149/200\n",
            "1291/1291 [==============================] - 26s 20ms/step - loss: 0.2102 - accuracy: 0.9243 - val_loss: 0.2818 - val_accuracy: 0.9136\n",
            "1229/1229 [==============================] - 8s 6ms/step - loss: 0.2744 - accuracy: 0.9125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.27438491582870483, 0.9125149250030518]"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "ZAGASJlUTYx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528d51ff-a1eb-4bd5-e752-1ad6b095db65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1229/1229 [==============================] - 7s 6ms/step - loss: 0.2744 - accuracy: 0.9125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.27438491582870483, 0.9125149250030518]"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    }
  ]
}