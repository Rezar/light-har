{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fee4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2738544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_metrics_base(model, x_test, y_test, model_path):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the PyTorch model.\n",
    "\n",
    "    :param model: PyTorch model.\n",
    "    :param x_test: Test dataset features (as a PyTorch Tensor).\n",
    "    :param y_test: Test dataset labels (as a NumPy array).\n",
    "    :param model_dir: Directory where the PyTorch model files are stored.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the model's predictions\n",
    "        outputs = model(x_test)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "        # Convert y_test to tensor if it's not already\n",
    "        true_labels = torch.tensor(y_test) if not isinstance(y_test, torch.Tensor) else y_test\n",
    "        true_labels = true_labels.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "    model_file = Path(model_path)\n",
    "\n",
    "    # Size in bytes\n",
    "    model_size_bytes = model_file.stat().st_size\n",
    "\n",
    "    # Convert size to kilobytes (optional)\n",
    "    model_size_kb = model_size_bytes / 1024\n",
    "    print(f\"Size of the model: {model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(true_labels.numpy(), predicted_labels.numpy())\n",
    "    print(f'Accuracy on the test set: {accuracy:.2%}')\n",
    "def measure_cpu_utilization_and_run(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measure CPU utilization while running a function.\n",
    "\n",
    "    Parameters:\n",
    "        func (function): The function to be executed.\n",
    "        *args: Arguments to be passed to func.\n",
    "        **kwargs: Keyword arguments to be passed to func.\n",
    "\n",
    "    Returns:\n",
    "        float: CPU utilization percentage during the execution of func.\n",
    "        float: The elapsed time during the execution of func.\n",
    "        any: The result of func execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Measure CPU utilization before execution\n",
    "    cpu_percent_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Execute the function and store its result\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Measure CPU utilization after execution\n",
    "    cpu_percent_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # Calculate elapsed time and average CPU utilization\n",
    "    elapsed_time = end_time - start_time\n",
    "    average_cpu_utilization = (cpu_percent_before + cpu_percent_after) / 2\n",
    "\n",
    "    return average_cpu_utilization, elapsed_time, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c2ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x = genfromtxt('../Data/WISDM_x.csv', delimiter=',')\n",
    "y_df = pd.read_csv('../Data/WISDM_y.csv')\n",
    "y = y_df.values.flatten()  # Flatten if y is 2D\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Function to create time series dataset\n",
    "def create_series(x, y, timestep, overlap):\n",
    "    slide_step = int(timestep * (1 - overlap))\n",
    "    data_num = int((len(x) / slide_step) - 1)\n",
    "    dataset = np.ndarray(shape=(data_num, timestep, x.shape[1]))\n",
    "    labels = []\n",
    "\n",
    "    for i in range(data_num):\n",
    "        labels.append(y[slide_step * (i + 1) - 1])\n",
    "        for j in range(timestep):\n",
    "            dataset[i, j, :] = x[slide_step * i + j, :]\n",
    "\n",
    "    return dataset, np.array(labels)\n",
    "\n",
    "# Create time series\n",
    "timestep = 16  # Replace with your value\n",
    "overlap = 0.5  # Replace with your value\n",
    "X_series, y_series = create_series(x, y_encoded, timestep, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7842f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:(109820, 16, 3), X_test shape:(27455, 16, 3), y_train shape:(109820,), y_test shape:(27455,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_series, y_series, test_size=0.2, random_state=42)\n",
    "print(f'X_train shape:{X_train.shape}, X_test shape:{X_test.shape}, y_train shape:{y_train.shape}, y_test shape:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86d475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)  # Assuming y_train is already encoded as class indexes\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d9c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b1440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, head_size, n_heads, ff_dim, dropout=0.0):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=n_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=ff_dim, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=input_dim, kernel_size=1)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # LayerNorm and Multi-head Attention\n",
    "        x = self.norm1(src)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x + src  # skip connection\n",
    "\n",
    "        # Feed Forward\n",
    "        x = self.norm2(x)\n",
    "        x = x.permute(1, 2, 0)  # Conv1D expects (batch_size, channels, length)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.permute(2, 0, 1)  # back to (length, batch_size, channels)\n",
    "        x = x + src  # skip connection\n",
    "        return x\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, sequence_length, num_features, head_size, n_heads, ff_dim, n_trans_blocks, mlp_units, drop=0.0, mlp_drop=0.0):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.encoders = nn.ModuleList([TransformerEncoderBlock(num_features, head_size, n_heads, ff_dim, drop) for _ in range(n_trans_blocks)])\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        mlp_layers = []\n",
    "        current_dim = num_features\n",
    "        for dim in mlp_units:\n",
    "            mlp_layers.append(nn.Linear(current_dim, dim))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(mlp_drop))\n",
    "            current_dim = dim  # Set input dim for the next layer\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        self.final_layer = nn.Linear(mlp_units[-1], 6)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, features)\n",
    "        for encoder in self.encoders:\n",
    "            src = encoder(src)\n",
    "\n",
    "        # Global average pooling\n",
    "        src = src.permute(1, 2, 0)  # pooling expects (batch_size, channels, length)\n",
    "        src = self.global_avg_pooling(src)\n",
    "        src = torch.flatten(src, 1)  # Flatten the output for the MLP\n",
    "\n",
    "        # MLP\n",
    "        src = self.mlp(src)\n",
    "        output = self.final_layer(src)\n",
    "        return output\n",
    "\n",
    "# Input parameters for your data\n",
    "sequence_length = 16  # The length of the time series sequences in your data\n",
    "num_features = 3     # The number of features in each time step of your data sequence\n",
    "\n",
    "# Instantiate the model\n",
    "# Instantiate the model with an adjusted number of heads and head size\n",
    "# The head size must be a multiple of num_features.\n",
    "model = TimeSeriesTransformer(\n",
    "    sequence_length=16, \n",
    "    num_features=3, \n",
    "    head_size=3,  # Each head will now have an embed size of 1 (3 / 3)\n",
    "    n_heads=1,  # Only one head since our embed_dim is 3\n",
    "    ff_dim=64, \n",
    "    n_trans_blocks=4, \n",
    "    mlp_units=[128, 64], \n",
    "    drop=0.1, \n",
    "    mlp_drop=0.1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bde95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (encoders): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderBlock(\n",
       "      (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 3, kernel_size=(1,), stride=(1,))\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (global_avg_pooling): AdaptiveAvgPool1d(output_size=1)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab86147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/3432], Loss: 1.3419, Accuracy: 48.47%\n",
      "Epoch [1/10], Step [200/3432], Loss: 1.1859, Accuracy: 57.31%\n",
      "Epoch [1/10], Step [300/3432], Loss: 1.0795, Accuracy: 63.03%\n",
      "Epoch [1/10], Step [400/3432], Loss: 0.9647, Accuracy: 67.78%\n",
      "Epoch [1/10], Step [500/3432], Loss: 0.8956, Accuracy: 70.47%\n",
      "Epoch [1/10], Step [600/3432], Loss: 0.8596, Accuracy: 72.94%\n",
      "Epoch [1/10], Step [700/3432], Loss: 0.8099, Accuracy: 72.69%\n",
      "Epoch [1/10], Step [800/3432], Loss: 0.7955, Accuracy: 73.19%\n",
      "Epoch [1/10], Step [900/3432], Loss: 0.7544, Accuracy: 74.09%\n",
      "Epoch [1/10], Step [1000/3432], Loss: 0.7499, Accuracy: 74.84%\n",
      "Epoch [1/10], Step [1100/3432], Loss: 0.7513, Accuracy: 74.44%\n",
      "Epoch [1/10], Step [1200/3432], Loss: 0.7958, Accuracy: 72.09%\n",
      "Epoch [1/10], Step [1300/3432], Loss: 0.7440, Accuracy: 73.91%\n",
      "Epoch [1/10], Step [1400/3432], Loss: 0.7140, Accuracy: 75.94%\n",
      "Epoch [1/10], Step [1500/3432], Loss: 0.7192, Accuracy: 75.22%\n",
      "Epoch [1/10], Step [1600/3432], Loss: 0.7131, Accuracy: 75.53%\n",
      "Epoch [1/10], Step [1700/3432], Loss: 0.7232, Accuracy: 74.81%\n",
      "Epoch [1/10], Step [1800/3432], Loss: 0.6871, Accuracy: 77.03%\n",
      "Epoch [1/10], Step [1900/3432], Loss: 0.7273, Accuracy: 75.72%\n",
      "Epoch [1/10], Step [2000/3432], Loss: 0.7166, Accuracy: 75.72%\n",
      "Epoch [1/10], Step [2100/3432], Loss: 0.7160, Accuracy: 75.78%\n",
      "Epoch [1/10], Step [2200/3432], Loss: 0.6902, Accuracy: 76.50%\n",
      "Epoch [1/10], Step [2300/3432], Loss: 0.7016, Accuracy: 76.00%\n",
      "Epoch [1/10], Step [2400/3432], Loss: 0.6772, Accuracy: 76.47%\n",
      "Epoch [1/10], Step [2500/3432], Loss: 0.7125, Accuracy: 75.38%\n",
      "Epoch [1/10], Step [2600/3432], Loss: 0.7107, Accuracy: 75.81%\n",
      "Epoch [1/10], Step [2700/3432], Loss: 0.7309, Accuracy: 74.31%\n",
      "Epoch [1/10], Step [2800/3432], Loss: 0.7100, Accuracy: 75.62%\n",
      "Epoch [1/10], Step [2900/3432], Loss: 0.7084, Accuracy: 75.75%\n",
      "Epoch [1/10], Step [3000/3432], Loss: 0.6813, Accuracy: 76.09%\n",
      "Epoch [1/10], Step [3100/3432], Loss: 0.6564, Accuracy: 78.50%\n",
      "Epoch [1/10], Step [3200/3432], Loss: 0.6736, Accuracy: 75.75%\n",
      "Epoch [1/10], Step [3300/3432], Loss: 0.6322, Accuracy: 77.81%\n",
      "Epoch [1/10], Step [3400/3432], Loss: 0.6800, Accuracy: 76.34%\n",
      "Epoch [2/10], Step [100/3432], Loss: 0.6570, Accuracy: 77.88%\n",
      "Epoch [2/10], Step [200/3432], Loss: 0.6757, Accuracy: 76.25%\n",
      "Epoch [2/10], Step [300/3432], Loss: 0.6817, Accuracy: 76.91%\n",
      "Epoch [2/10], Step [400/3432], Loss: 0.6908, Accuracy: 75.94%\n",
      "Epoch [2/10], Step [500/3432], Loss: 0.6516, Accuracy: 78.03%\n",
      "Epoch [2/10], Step [600/3432], Loss: 0.6635, Accuracy: 77.34%\n",
      "Epoch [2/10], Step [700/3432], Loss: 0.6465, Accuracy: 78.81%\n",
      "Epoch [2/10], Step [800/3432], Loss: 0.6632, Accuracy: 77.69%\n",
      "Epoch [2/10], Step [900/3432], Loss: 0.6744, Accuracy: 76.78%\n",
      "Epoch [2/10], Step [1000/3432], Loss: 0.6711, Accuracy: 76.62%\n",
      "Epoch [2/10], Step [1100/3432], Loss: 0.6808, Accuracy: 75.88%\n",
      "Epoch [2/10], Step [1200/3432], Loss: 0.6468, Accuracy: 77.03%\n",
      "Epoch [2/10], Step [1300/3432], Loss: 0.6557, Accuracy: 77.22%\n",
      "Epoch [2/10], Step [1400/3432], Loss: 0.6546, Accuracy: 77.12%\n",
      "Epoch [2/10], Step [1500/3432], Loss: 0.6466, Accuracy: 77.72%\n",
      "Epoch [2/10], Step [1600/3432], Loss: 0.6620, Accuracy: 76.88%\n",
      "Epoch [2/10], Step [1700/3432], Loss: 0.6584, Accuracy: 77.84%\n",
      "Epoch [2/10], Step [1800/3432], Loss: 0.6250, Accuracy: 79.03%\n",
      "Epoch [2/10], Step [1900/3432], Loss: 0.6692, Accuracy: 77.75%\n",
      "Epoch [2/10], Step [2000/3432], Loss: 0.6180, Accuracy: 78.47%\n",
      "Epoch [2/10], Step [2100/3432], Loss: 0.6554, Accuracy: 77.12%\n",
      "Epoch [2/10], Step [2200/3432], Loss: 0.6339, Accuracy: 78.09%\n",
      "Epoch [2/10], Step [2300/3432], Loss: 0.6401, Accuracy: 77.06%\n",
      "Epoch [2/10], Step [2400/3432], Loss: 0.6357, Accuracy: 77.03%\n",
      "Epoch [2/10], Step [2500/3432], Loss: 0.6115, Accuracy: 79.47%\n",
      "Epoch [2/10], Step [2600/3432], Loss: 0.6295, Accuracy: 78.38%\n",
      "Epoch [2/10], Step [2700/3432], Loss: 0.6560, Accuracy: 77.19%\n",
      "Epoch [2/10], Step [2800/3432], Loss: 0.6533, Accuracy: 77.38%\n",
      "Epoch [2/10], Step [2900/3432], Loss: 0.6722, Accuracy: 77.50%\n",
      "Epoch [2/10], Step [3000/3432], Loss: 0.6411, Accuracy: 77.50%\n",
      "Epoch [2/10], Step [3100/3432], Loss: 0.6476, Accuracy: 77.78%\n",
      "Epoch [2/10], Step [3200/3432], Loss: 0.6349, Accuracy: 77.94%\n",
      "Epoch [2/10], Step [3300/3432], Loss: 0.6352, Accuracy: 78.31%\n",
      "Epoch [2/10], Step [3400/3432], Loss: 0.6626, Accuracy: 76.84%\n",
      "Epoch [3/10], Step [100/3432], Loss: 0.6074, Accuracy: 79.12%\n",
      "Epoch [3/10], Step [200/3432], Loss: 0.6622, Accuracy: 77.00%\n",
      "Epoch [3/10], Step [300/3432], Loss: 0.6302, Accuracy: 78.19%\n",
      "Epoch [3/10], Step [400/3432], Loss: 0.6442, Accuracy: 78.34%\n",
      "Epoch [3/10], Step [500/3432], Loss: 0.6311, Accuracy: 78.91%\n",
      "Epoch [3/10], Step [600/3432], Loss: 0.6279, Accuracy: 78.12%\n",
      "Epoch [3/10], Step [700/3432], Loss: 0.6205, Accuracy: 78.47%\n",
      "Epoch [3/10], Step [800/3432], Loss: 0.6239, Accuracy: 78.44%\n",
      "Epoch [3/10], Step [900/3432], Loss: 0.6352, Accuracy: 77.78%\n",
      "Epoch [3/10], Step [1000/3432], Loss: 0.6328, Accuracy: 77.88%\n",
      "Epoch [3/10], Step [1100/3432], Loss: 0.6491, Accuracy: 78.00%\n",
      "Epoch [3/10], Step [1200/3432], Loss: 0.6110, Accuracy: 78.72%\n",
      "Epoch [3/10], Step [1300/3432], Loss: 0.6445, Accuracy: 77.53%\n",
      "Epoch [3/10], Step [1400/3432], Loss: 0.6478, Accuracy: 77.78%\n",
      "Epoch [3/10], Step [1500/3432], Loss: 0.6368, Accuracy: 77.44%\n",
      "Epoch [3/10], Step [1600/3432], Loss: 0.6377, Accuracy: 77.44%\n",
      "Epoch [3/10], Step [1700/3432], Loss: 0.6331, Accuracy: 78.47%\n",
      "Epoch [3/10], Step [1800/3432], Loss: 0.5927, Accuracy: 79.47%\n",
      "Epoch [3/10], Step [1900/3432], Loss: 0.6052, Accuracy: 80.16%\n",
      "Epoch [3/10], Step [2000/3432], Loss: 0.6029, Accuracy: 79.56%\n",
      "Epoch [3/10], Step [2100/3432], Loss: 0.6685, Accuracy: 76.66%\n",
      "Epoch [3/10], Step [2200/3432], Loss: 0.6155, Accuracy: 78.19%\n",
      "Epoch [3/10], Step [2300/3432], Loss: 0.6314, Accuracy: 78.31%\n",
      "Epoch [3/10], Step [2400/3432], Loss: 0.6072, Accuracy: 78.22%\n",
      "Epoch [3/10], Step [2500/3432], Loss: 0.6464, Accuracy: 78.09%\n",
      "Epoch [3/10], Step [2600/3432], Loss: 0.6335, Accuracy: 78.25%\n",
      "Epoch [3/10], Step [2700/3432], Loss: 0.6101, Accuracy: 78.81%\n",
      "Epoch [3/10], Step [2800/3432], Loss: 0.6263, Accuracy: 77.66%\n",
      "Epoch [3/10], Step [2900/3432], Loss: 0.6187, Accuracy: 78.22%\n",
      "Epoch [3/10], Step [3000/3432], Loss: 0.6155, Accuracy: 78.62%\n",
      "Epoch [3/10], Step [3100/3432], Loss: 0.6347, Accuracy: 77.06%\n",
      "Epoch [3/10], Step [3200/3432], Loss: 0.6086, Accuracy: 78.59%\n",
      "Epoch [3/10], Step [3300/3432], Loss: 0.6041, Accuracy: 78.94%\n",
      "Epoch [3/10], Step [3400/3432], Loss: 0.6137, Accuracy: 78.50%\n",
      "Epoch [4/10], Step [100/3432], Loss: 0.6091, Accuracy: 79.50%\n",
      "Epoch [4/10], Step [200/3432], Loss: 0.6059, Accuracy: 80.09%\n",
      "Epoch [4/10], Step [300/3432], Loss: 0.6297, Accuracy: 78.41%\n",
      "Epoch [4/10], Step [400/3432], Loss: 0.6360, Accuracy: 77.91%\n",
      "Epoch [4/10], Step [500/3432], Loss: 0.6020, Accuracy: 79.12%\n",
      "Epoch [4/10], Step [600/3432], Loss: 0.6261, Accuracy: 77.84%\n",
      "Epoch [4/10], Step [700/3432], Loss: 0.5836, Accuracy: 79.50%\n",
      "Epoch [4/10], Step [800/3432], Loss: 0.6227, Accuracy: 77.69%\n",
      "Epoch [4/10], Step [900/3432], Loss: 0.6033, Accuracy: 79.06%\n",
      "Epoch [4/10], Step [1000/3432], Loss: 0.6555, Accuracy: 77.03%\n",
      "Epoch [4/10], Step [1100/3432], Loss: 0.6110, Accuracy: 78.91%\n",
      "Epoch [4/10], Step [1200/3432], Loss: 0.6327, Accuracy: 77.84%\n",
      "Epoch [4/10], Step [1300/3432], Loss: 0.6157, Accuracy: 78.56%\n",
      "Epoch [4/10], Step [1400/3432], Loss: 0.6156, Accuracy: 78.34%\n",
      "Epoch [4/10], Step [1500/3432], Loss: 0.6094, Accuracy: 78.84%\n",
      "Epoch [4/10], Step [1600/3432], Loss: 0.6328, Accuracy: 78.38%\n",
      "Epoch [4/10], Step [1700/3432], Loss: 0.6011, Accuracy: 79.00%\n",
      "Epoch [4/10], Step [1800/3432], Loss: 0.6206, Accuracy: 78.38%\n",
      "Epoch [4/10], Step [1900/3432], Loss: 0.6088, Accuracy: 78.59%\n",
      "Epoch [4/10], Step [2000/3432], Loss: 0.6431, Accuracy: 77.38%\n",
      "Epoch [4/10], Step [2100/3432], Loss: 0.5871, Accuracy: 78.69%\n",
      "Epoch [4/10], Step [2200/3432], Loss: 0.6232, Accuracy: 78.41%\n",
      "Epoch [4/10], Step [2300/3432], Loss: 0.5979, Accuracy: 79.12%\n",
      "Epoch [4/10], Step [2400/3432], Loss: 0.5811, Accuracy: 80.16%\n",
      "Epoch [4/10], Step [2500/3432], Loss: 0.5825, Accuracy: 79.91%\n",
      "Epoch [4/10], Step [2600/3432], Loss: 0.6156, Accuracy: 78.31%\n",
      "Epoch [4/10], Step [2700/3432], Loss: 0.5997, Accuracy: 78.53%\n",
      "Epoch [4/10], Step [2800/3432], Loss: 0.5984, Accuracy: 79.25%\n",
      "Epoch [4/10], Step [2900/3432], Loss: 0.6099, Accuracy: 78.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [3000/3432], Loss: 0.6158, Accuracy: 78.12%\n",
      "Epoch [4/10], Step [3100/3432], Loss: 0.6059, Accuracy: 78.94%\n",
      "Epoch [4/10], Step [3200/3432], Loss: 0.6153, Accuracy: 78.56%\n",
      "Epoch [4/10], Step [3300/3432], Loss: 0.5908, Accuracy: 78.84%\n",
      "Epoch [4/10], Step [3400/3432], Loss: 0.6187, Accuracy: 78.25%\n",
      "Epoch [5/10], Step [100/3432], Loss: 0.6323, Accuracy: 78.16%\n",
      "Epoch [5/10], Step [200/3432], Loss: 0.6301, Accuracy: 78.50%\n",
      "Epoch [5/10], Step [300/3432], Loss: 0.6310, Accuracy: 77.50%\n",
      "Epoch [5/10], Step [400/3432], Loss: 0.5925, Accuracy: 79.06%\n",
      "Epoch [5/10], Step [500/3432], Loss: 0.5740, Accuracy: 80.28%\n",
      "Epoch [5/10], Step [600/3432], Loss: 0.6154, Accuracy: 78.84%\n",
      "Epoch [5/10], Step [700/3432], Loss: 0.5966, Accuracy: 79.38%\n",
      "Epoch [5/10], Step [800/3432], Loss: 0.5926, Accuracy: 78.72%\n",
      "Epoch [5/10], Step [900/3432], Loss: 0.6111, Accuracy: 79.38%\n",
      "Epoch [5/10], Step [1000/3432], Loss: 0.5799, Accuracy: 79.47%\n",
      "Epoch [5/10], Step [1100/3432], Loss: 0.5950, Accuracy: 79.06%\n",
      "Epoch [5/10], Step [1200/3432], Loss: 0.6018, Accuracy: 78.97%\n",
      "Epoch [5/10], Step [1300/3432], Loss: 0.6092, Accuracy: 78.12%\n",
      "Epoch [5/10], Step [1400/3432], Loss: 0.5978, Accuracy: 78.88%\n",
      "Epoch [5/10], Step [1500/3432], Loss: 0.6005, Accuracy: 78.69%\n",
      "Epoch [5/10], Step [1600/3432], Loss: 0.5900, Accuracy: 79.53%\n",
      "Epoch [5/10], Step [1700/3432], Loss: 0.6156, Accuracy: 79.03%\n",
      "Epoch [5/10], Step [1800/3432], Loss: 0.6211, Accuracy: 79.06%\n",
      "Epoch [5/10], Step [1900/3432], Loss: 0.6229, Accuracy: 78.34%\n",
      "Epoch [5/10], Step [2000/3432], Loss: 0.6031, Accuracy: 78.75%\n",
      "Epoch [5/10], Step [2100/3432], Loss: 0.5879, Accuracy: 79.19%\n",
      "Epoch [5/10], Step [2200/3432], Loss: 0.5687, Accuracy: 80.00%\n",
      "Epoch [5/10], Step [2300/3432], Loss: 0.6225, Accuracy: 78.59%\n",
      "Epoch [5/10], Step [2400/3432], Loss: 0.6045, Accuracy: 79.44%\n",
      "Epoch [5/10], Step [2500/3432], Loss: 0.6046, Accuracy: 78.66%\n",
      "Epoch [5/10], Step [2600/3432], Loss: 0.5984, Accuracy: 78.78%\n",
      "Epoch [5/10], Step [2700/3432], Loss: 0.6284, Accuracy: 78.00%\n",
      "Epoch [5/10], Step [2800/3432], Loss: 0.5964, Accuracy: 79.59%\n",
      "Epoch [5/10], Step [2900/3432], Loss: 0.5688, Accuracy: 80.47%\n",
      "Epoch [5/10], Step [3000/3432], Loss: 0.6095, Accuracy: 79.06%\n",
      "Epoch [5/10], Step [3100/3432], Loss: 0.5883, Accuracy: 79.00%\n",
      "Epoch [5/10], Step [3200/3432], Loss: 0.5927, Accuracy: 79.84%\n",
      "Epoch [5/10], Step [3300/3432], Loss: 0.5954, Accuracy: 79.06%\n",
      "Epoch [5/10], Step [3400/3432], Loss: 0.5555, Accuracy: 80.72%\n",
      "Epoch [6/10], Step [100/3432], Loss: 0.5985, Accuracy: 78.75%\n",
      "Epoch [6/10], Step [200/3432], Loss: 0.5805, Accuracy: 79.28%\n",
      "Epoch [6/10], Step [300/3432], Loss: 0.5871, Accuracy: 80.06%\n",
      "Epoch [6/10], Step [400/3432], Loss: 0.5801, Accuracy: 79.69%\n",
      "Epoch [6/10], Step [500/3432], Loss: 0.5857, Accuracy: 79.84%\n",
      "Epoch [6/10], Step [600/3432], Loss: 0.5780, Accuracy: 79.81%\n",
      "Epoch [6/10], Step [700/3432], Loss: 0.6262, Accuracy: 78.16%\n",
      "Epoch [6/10], Step [800/3432], Loss: 0.5782, Accuracy: 79.69%\n",
      "Epoch [6/10], Step [900/3432], Loss: 0.5938, Accuracy: 79.19%\n",
      "Epoch [6/10], Step [1000/3432], Loss: 0.6037, Accuracy: 78.53%\n",
      "Epoch [6/10], Step [1100/3432], Loss: 0.6207, Accuracy: 77.97%\n",
      "Epoch [6/10], Step [1200/3432], Loss: 0.6150, Accuracy: 78.41%\n",
      "Epoch [6/10], Step [1300/3432], Loss: 0.6124, Accuracy: 78.78%\n",
      "Epoch [6/10], Step [1400/3432], Loss: 0.5942, Accuracy: 78.97%\n",
      "Epoch [6/10], Step [1500/3432], Loss: 0.6017, Accuracy: 79.66%\n",
      "Epoch [6/10], Step [1600/3432], Loss: 0.5789, Accuracy: 80.41%\n",
      "Epoch [6/10], Step [1700/3432], Loss: 0.5927, Accuracy: 79.59%\n",
      "Epoch [6/10], Step [1800/3432], Loss: 0.5976, Accuracy: 79.78%\n",
      "Epoch [6/10], Step [1900/3432], Loss: 0.6047, Accuracy: 79.56%\n",
      "Epoch [6/10], Step [2000/3432], Loss: 0.6055, Accuracy: 78.88%\n",
      "Epoch [6/10], Step [2100/3432], Loss: 0.5926, Accuracy: 80.22%\n",
      "Epoch [6/10], Step [2200/3432], Loss: 0.5928, Accuracy: 79.34%\n",
      "Epoch [6/10], Step [2300/3432], Loss: 0.5925, Accuracy: 80.19%\n",
      "Epoch [6/10], Step [2400/3432], Loss: 0.6064, Accuracy: 79.22%\n",
      "Epoch [6/10], Step [2500/3432], Loss: 0.5882, Accuracy: 79.22%\n",
      "Epoch [6/10], Step [2600/3432], Loss: 0.5683, Accuracy: 80.22%\n",
      "Epoch [6/10], Step [2700/3432], Loss: 0.6055, Accuracy: 79.44%\n",
      "Epoch [6/10], Step [2800/3432], Loss: 0.6056, Accuracy: 79.06%\n",
      "Epoch [6/10], Step [2900/3432], Loss: 0.5764, Accuracy: 79.66%\n",
      "Epoch [6/10], Step [3000/3432], Loss: 0.5626, Accuracy: 80.56%\n",
      "Epoch [6/10], Step [3100/3432], Loss: 0.6046, Accuracy: 79.00%\n",
      "Epoch [6/10], Step [3200/3432], Loss: 0.5856, Accuracy: 79.28%\n",
      "Epoch [6/10], Step [3300/3432], Loss: 0.5934, Accuracy: 79.47%\n",
      "Epoch [6/10], Step [3400/3432], Loss: 0.5855, Accuracy: 80.12%\n",
      "Epoch [7/10], Step [100/3432], Loss: 0.5637, Accuracy: 80.19%\n",
      "Epoch [7/10], Step [200/3432], Loss: 0.5813, Accuracy: 80.22%\n",
      "Epoch [7/10], Step [300/3432], Loss: 0.6181, Accuracy: 78.38%\n",
      "Epoch [7/10], Step [400/3432], Loss: 0.5916, Accuracy: 80.06%\n",
      "Epoch [7/10], Step [500/3432], Loss: 0.5912, Accuracy: 79.16%\n",
      "Epoch [7/10], Step [600/3432], Loss: 0.6113, Accuracy: 78.47%\n",
      "Epoch [7/10], Step [700/3432], Loss: 0.5764, Accuracy: 81.22%\n",
      "Epoch [7/10], Step [800/3432], Loss: 0.5715, Accuracy: 80.12%\n",
      "Epoch [7/10], Step [900/3432], Loss: 0.5728, Accuracy: 80.38%\n",
      "Epoch [7/10], Step [1000/3432], Loss: 0.5642, Accuracy: 80.59%\n",
      "Epoch [7/10], Step [1100/3432], Loss: 0.6185, Accuracy: 78.91%\n",
      "Epoch [7/10], Step [1200/3432], Loss: 0.5654, Accuracy: 80.19%\n",
      "Epoch [7/10], Step [1300/3432], Loss: 0.5793, Accuracy: 80.44%\n",
      "Epoch [7/10], Step [1400/3432], Loss: 0.5693, Accuracy: 79.72%\n",
      "Epoch [7/10], Step [1500/3432], Loss: 0.6141, Accuracy: 78.59%\n",
      "Epoch [7/10], Step [1600/3432], Loss: 0.5669, Accuracy: 80.16%\n",
      "Epoch [7/10], Step [1700/3432], Loss: 0.5950, Accuracy: 79.34%\n",
      "Epoch [7/10], Step [1800/3432], Loss: 0.5666, Accuracy: 80.06%\n",
      "Epoch [7/10], Step [1900/3432], Loss: 0.5894, Accuracy: 79.75%\n",
      "Epoch [7/10], Step [2000/3432], Loss: 0.5880, Accuracy: 79.47%\n",
      "Epoch [7/10], Step [2100/3432], Loss: 0.5852, Accuracy: 79.44%\n",
      "Epoch [7/10], Step [2200/3432], Loss: 0.5523, Accuracy: 81.22%\n",
      "Epoch [7/10], Step [2300/3432], Loss: 0.5662, Accuracy: 80.69%\n",
      "Epoch [7/10], Step [2400/3432], Loss: 0.5835, Accuracy: 79.88%\n",
      "Epoch [7/10], Step [2500/3432], Loss: 0.5894, Accuracy: 79.12%\n",
      "Epoch [7/10], Step [2600/3432], Loss: 0.5847, Accuracy: 79.03%\n",
      "Epoch [7/10], Step [2700/3432], Loss: 0.5726, Accuracy: 80.31%\n",
      "Epoch [7/10], Step [2800/3432], Loss: 0.5996, Accuracy: 80.25%\n",
      "Epoch [7/10], Step [2900/3432], Loss: 0.5664, Accuracy: 80.94%\n",
      "Epoch [7/10], Step [3000/3432], Loss: 0.5795, Accuracy: 80.03%\n",
      "Epoch [7/10], Step [3100/3432], Loss: 0.5870, Accuracy: 79.59%\n",
      "Epoch [7/10], Step [3200/3432], Loss: 0.5738, Accuracy: 80.38%\n",
      "Epoch [7/10], Step [3300/3432], Loss: 0.5829, Accuracy: 79.69%\n",
      "Epoch [7/10], Step [3400/3432], Loss: 0.5912, Accuracy: 79.88%\n",
      "Epoch [8/10], Step [100/3432], Loss: 0.5850, Accuracy: 80.50%\n",
      "Epoch [8/10], Step [200/3432], Loss: 0.6010, Accuracy: 78.69%\n",
      "Epoch [8/10], Step [300/3432], Loss: 0.5989, Accuracy: 78.94%\n",
      "Epoch [8/10], Step [400/3432], Loss: 0.5685, Accuracy: 80.12%\n",
      "Epoch [8/10], Step [500/3432], Loss: 0.5922, Accuracy: 78.78%\n",
      "Epoch [8/10], Step [600/3432], Loss: 0.5820, Accuracy: 80.78%\n",
      "Epoch [8/10], Step [700/3432], Loss: 0.5588, Accuracy: 80.94%\n",
      "Epoch [8/10], Step [800/3432], Loss: 0.5996, Accuracy: 79.91%\n",
      "Epoch [8/10], Step [900/3432], Loss: 0.6084, Accuracy: 79.28%\n",
      "Epoch [8/10], Step [1000/3432], Loss: 0.5739, Accuracy: 80.09%\n",
      "Epoch [8/10], Step [1100/3432], Loss: 0.6172, Accuracy: 78.28%\n",
      "Epoch [8/10], Step [1200/3432], Loss: 0.5800, Accuracy: 80.34%\n",
      "Epoch [8/10], Step [1300/3432], Loss: 0.6026, Accuracy: 79.16%\n",
      "Epoch [8/10], Step [1400/3432], Loss: 0.5857, Accuracy: 79.41%\n",
      "Epoch [8/10], Step [1500/3432], Loss: 0.5544, Accuracy: 81.03%\n",
      "Epoch [8/10], Step [1600/3432], Loss: 0.5563, Accuracy: 81.00%\n",
      "Epoch [8/10], Step [1700/3432], Loss: 0.6014, Accuracy: 79.44%\n",
      "Epoch [8/10], Step [1800/3432], Loss: 0.5942, Accuracy: 79.31%\n",
      "Epoch [8/10], Step [1900/3432], Loss: 0.5822, Accuracy: 79.78%\n",
      "Epoch [8/10], Step [2000/3432], Loss: 0.5898, Accuracy: 79.59%\n",
      "Epoch [8/10], Step [2100/3432], Loss: 0.5720, Accuracy: 80.47%\n",
      "Epoch [8/10], Step [2200/3432], Loss: 0.5750, Accuracy: 80.12%\n",
      "Epoch [8/10], Step [2300/3432], Loss: 0.5814, Accuracy: 79.34%\n",
      "Epoch [8/10], Step [2400/3432], Loss: 0.5638, Accuracy: 80.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [2500/3432], Loss: 0.5771, Accuracy: 80.03%\n",
      "Epoch [8/10], Step [2600/3432], Loss: 0.5691, Accuracy: 79.91%\n",
      "Epoch [8/10], Step [2700/3432], Loss: 0.5755, Accuracy: 80.84%\n",
      "Epoch [8/10], Step [2800/3432], Loss: 0.5608, Accuracy: 80.38%\n",
      "Epoch [8/10], Step [2900/3432], Loss: 0.5671, Accuracy: 80.53%\n",
      "Epoch [8/10], Step [3000/3432], Loss: 0.5634, Accuracy: 79.81%\n",
      "Epoch [8/10], Step [3100/3432], Loss: 0.5831, Accuracy: 79.88%\n",
      "Epoch [8/10], Step [3200/3432], Loss: 0.5467, Accuracy: 80.75%\n",
      "Epoch [8/10], Step [3300/3432], Loss: 0.5771, Accuracy: 80.03%\n",
      "Epoch [8/10], Step [3400/3432], Loss: 0.5851, Accuracy: 79.28%\n",
      "Epoch [9/10], Step [100/3432], Loss: 0.5721, Accuracy: 80.38%\n",
      "Epoch [9/10], Step [200/3432], Loss: 0.5620, Accuracy: 79.84%\n",
      "Epoch [9/10], Step [300/3432], Loss: 0.5900, Accuracy: 79.41%\n",
      "Epoch [9/10], Step [400/3432], Loss: 0.5784, Accuracy: 80.25%\n",
      "Epoch [9/10], Step [500/3432], Loss: 0.5652, Accuracy: 80.34%\n",
      "Epoch [9/10], Step [600/3432], Loss: 0.5818, Accuracy: 79.94%\n",
      "Epoch [9/10], Step [700/3432], Loss: 0.5902, Accuracy: 79.28%\n",
      "Epoch [9/10], Step [800/3432], Loss: 0.5548, Accuracy: 80.91%\n",
      "Epoch [9/10], Step [900/3432], Loss: 0.5389, Accuracy: 81.28%\n",
      "Epoch [9/10], Step [1000/3432], Loss: 0.5679, Accuracy: 80.31%\n",
      "Epoch [9/10], Step [1100/3432], Loss: 0.6043, Accuracy: 78.53%\n",
      "Epoch [9/10], Step [1200/3432], Loss: 0.5467, Accuracy: 80.94%\n",
      "Epoch [9/10], Step [1300/3432], Loss: 0.5678, Accuracy: 80.47%\n",
      "Epoch [9/10], Step [1400/3432], Loss: 0.5570, Accuracy: 81.56%\n",
      "Epoch [9/10], Step [1500/3432], Loss: 0.5716, Accuracy: 80.94%\n",
      "Epoch [9/10], Step [1600/3432], Loss: 0.5506, Accuracy: 81.72%\n",
      "Epoch [9/10], Step [1700/3432], Loss: 0.5590, Accuracy: 81.06%\n",
      "Epoch [9/10], Step [1800/3432], Loss: 0.5875, Accuracy: 79.38%\n",
      "Epoch [9/10], Step [1900/3432], Loss: 0.5774, Accuracy: 80.31%\n",
      "Epoch [9/10], Step [2000/3432], Loss: 0.5527, Accuracy: 80.69%\n",
      "Epoch [9/10], Step [2100/3432], Loss: 0.5779, Accuracy: 79.47%\n",
      "Epoch [9/10], Step [2200/3432], Loss: 0.5468, Accuracy: 81.19%\n",
      "Epoch [9/10], Step [2300/3432], Loss: 0.5747, Accuracy: 80.34%\n",
      "Epoch [9/10], Step [2400/3432], Loss: 0.5605, Accuracy: 81.19%\n",
      "Epoch [9/10], Step [2500/3432], Loss: 0.5907, Accuracy: 79.28%\n",
      "Epoch [9/10], Step [2600/3432], Loss: 0.5828, Accuracy: 79.81%\n",
      "Epoch [9/10], Step [2700/3432], Loss: 0.5880, Accuracy: 79.31%\n",
      "Epoch [9/10], Step [2800/3432], Loss: 0.5678, Accuracy: 80.69%\n",
      "Epoch [9/10], Step [2900/3432], Loss: 0.5786, Accuracy: 79.69%\n",
      "Epoch [9/10], Step [3000/3432], Loss: 0.5992, Accuracy: 78.97%\n",
      "Epoch [9/10], Step [3100/3432], Loss: 0.5533, Accuracy: 80.69%\n",
      "Epoch [9/10], Step [3200/3432], Loss: 0.5652, Accuracy: 80.50%\n",
      "Epoch [9/10], Step [3300/3432], Loss: 0.5721, Accuracy: 79.78%\n",
      "Epoch [9/10], Step [3400/3432], Loss: 0.5732, Accuracy: 80.12%\n",
      "Epoch [10/10], Step [100/3432], Loss: 0.5629, Accuracy: 80.69%\n",
      "Epoch [10/10], Step [200/3432], Loss: 0.5797, Accuracy: 80.28%\n",
      "Epoch [10/10], Step [300/3432], Loss: 0.5371, Accuracy: 81.34%\n",
      "Epoch [10/10], Step [400/3432], Loss: 0.5785, Accuracy: 79.84%\n",
      "Epoch [10/10], Step [500/3432], Loss: 0.5517, Accuracy: 81.12%\n",
      "Epoch [10/10], Step [600/3432], Loss: 0.5664, Accuracy: 80.72%\n",
      "Epoch [10/10], Step [700/3432], Loss: 0.5771, Accuracy: 80.16%\n",
      "Epoch [10/10], Step [800/3432], Loss: 0.5562, Accuracy: 81.06%\n",
      "Epoch [10/10], Step [900/3432], Loss: 0.5778, Accuracy: 80.16%\n",
      "Epoch [10/10], Step [1000/3432], Loss: 0.5720, Accuracy: 80.12%\n",
      "Epoch [10/10], Step [1100/3432], Loss: 0.5555, Accuracy: 80.84%\n",
      "Epoch [10/10], Step [1200/3432], Loss: 0.5880, Accuracy: 80.06%\n",
      "Epoch [10/10], Step [1300/3432], Loss: 0.5622, Accuracy: 80.06%\n",
      "Epoch [10/10], Step [1400/3432], Loss: 0.5649, Accuracy: 80.25%\n",
      "Epoch [10/10], Step [1500/3432], Loss: 0.5511, Accuracy: 81.09%\n",
      "Epoch [10/10], Step [1600/3432], Loss: 0.5605, Accuracy: 80.94%\n",
      "Epoch [10/10], Step [1700/3432], Loss: 0.5797, Accuracy: 79.62%\n",
      "Epoch [10/10], Step [1800/3432], Loss: 0.5572, Accuracy: 81.34%\n",
      "Epoch [10/10], Step [1900/3432], Loss: 0.5706, Accuracy: 80.53%\n",
      "Epoch [10/10], Step [2000/3432], Loss: 0.5587, Accuracy: 80.41%\n",
      "Epoch [10/10], Step [2100/3432], Loss: 0.5496, Accuracy: 81.00%\n",
      "Epoch [10/10], Step [2200/3432], Loss: 0.5741, Accuracy: 80.34%\n",
      "Epoch [10/10], Step [2300/3432], Loss: 0.5769, Accuracy: 80.25%\n",
      "Epoch [10/10], Step [2400/3432], Loss: 0.5766, Accuracy: 80.06%\n",
      "Epoch [10/10], Step [2500/3432], Loss: 0.5855, Accuracy: 80.06%\n",
      "Epoch [10/10], Step [2600/3432], Loss: 0.5876, Accuracy: 79.28%\n",
      "Epoch [10/10], Step [2700/3432], Loss: 0.5901, Accuracy: 80.09%\n",
      "Epoch [10/10], Step [2800/3432], Loss: 0.5379, Accuracy: 81.97%\n",
      "Epoch [10/10], Step [2900/3432], Loss: 0.5683, Accuracy: 80.66%\n",
      "Epoch [10/10], Step [3000/3432], Loss: 0.5729, Accuracy: 79.53%\n",
      "Epoch [10/10], Step [3100/3432], Loss: 0.5799, Accuracy: 80.38%\n",
      "Epoch [10/10], Step [3200/3432], Loss: 0.5376, Accuracy: 81.28%\n",
      "Epoch [10/10], Step [3300/3432], Loss: 0.5686, Accuracy: 80.22%\n",
      "Epoch [10/10], Step [3400/3432], Loss: 0.5487, Accuracy: 80.44%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n",
    "                  f'Loss: {running_loss / 100:.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "# After training, you may want to save your model\n",
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "365f1ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8076\n",
      "Precision: 0.7903\n",
      "Recall: 0.8076\n",
      "F1 Score: 0.7821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Collect all predictions and labels to compute overall metrics\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert collected predictions and labels to arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cbb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd3612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 61.19 KB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Assume 'model' is the instance of TimeSeriesTransformer you have already defined and trained\n",
    "model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/transformer_base.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Get the size of the saved model file\n",
    "model_size = os.path.getsize(model_path)\n",
    "print(f\"The model size is {model_size/1024:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b340fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 61.19 KB\n",
      "Accuracy on the test set: 80.76%\n",
      "CPU usage during inference: 39.80%\n",
      "Inference time: 1.5054 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model, X_test_tensor, y_test_tensor, model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38360f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch.quantization\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "# Load the saved model's state dict\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Make sure the model is in evaluation mode before quantization\n",
    "model.eval()\n",
    "\n",
    "# Perform dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    {torch.nn.Linear},  # specify which layer types to quantize\n",
    "    dtype=torch.qint8  # the target data type for quantized weights\n",
    ")\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/transformer_quantized.pth\"\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)\n",
    "\n",
    "# Now you can use quantized_model for inference\n",
    "print(\"Quantization complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f4ebff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of quantized model: 37.35 KB\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the quantized model\n",
    "quantized_model_size = os.path.getsize(quantized_model_path)\n",
    "print(f\"Size of quantized model: {quantized_model_size/1024:0.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0bc8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W qlinear_dynamic.cpp:247] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8080\n",
      "Precision: 0.7909\n",
      "Recall: 0.8080\n",
      "F1 Score: 0.7822\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "quantized_model.eval()  # Set the model to evaluation mode\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = quantized_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Collect all predictions and labels to compute overall metrics\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert collected predictions and labels to arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ee39d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 37.35 KB\n",
      "Accuracy on the test set: 80.66%\n",
      "CPU usage during inference: 35.75%\n",
      "Inference time: 1.4863 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, quantized_model, X_test_tensor, y_test_tensor, quantized_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f6354",
   "metadata": {},
   "source": [
    "### Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0acad4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, head_size, n_heads, ff_dim, dropout=0.0):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=n_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=ff_dim, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=input_dim, kernel_size=1)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # LayerNorm and Multi-head Attention\n",
    "        x = self.norm1(src)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x + src  # skip connection\n",
    "\n",
    "        # Feed Forward\n",
    "        x = self.norm2(x)\n",
    "        x = x.permute(1, 2, 0)  # Conv1D expects (batch_size, channels, length)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.permute(2, 0, 1)  # back to (length, batch_size, channels)\n",
    "        x = x + src  # skip connection\n",
    "        return x\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, sequence_length, num_features, head_size, n_heads, ff_dim, n_trans_blocks, mlp_units, drop=0.0, mlp_drop=0.0):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.encoders = nn.ModuleList([TransformerEncoderBlock(num_features, head_size, n_heads, ff_dim, drop) for _ in range(n_trans_blocks)])\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        mlp_layers = []\n",
    "        current_dim = num_features\n",
    "        for dim in mlp_units:\n",
    "            mlp_layers.append(nn.Linear(current_dim, dim))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(mlp_drop))\n",
    "            current_dim = dim  # Set input dim for the next layer\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        self.final_layer = nn.Linear(mlp_units[-1], 6)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.quant(src)\n",
    "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, features)\n",
    "        for encoder in self.encoders:\n",
    "            src = encoder(src)\n",
    "\n",
    "        # Global average pooling\n",
    "        src = src.permute(1, 2, 0)  # pooling expects (batch_size, channels, length)\n",
    "        src = self.global_avg_pooling(src)\n",
    "        src = torch.flatten(src, 1)  # Flatten the output for the MLP\n",
    "\n",
    "        # MLP\n",
    "        src = self.mlp(src)\n",
    "        src = self.final_layer(src)\n",
    "        src = self.dequant(src)\n",
    "        return src\n",
    "\n",
    "# Input parameters for your data\n",
    "sequence_length = 16  # The length of the time series sequences in your data\n",
    "num_features = 3     # The number of features in each time step of your data sequence\n",
    "\n",
    "# Instantiate the model\n",
    "# Instantiate the model with an adjusted number of heads and head size\n",
    "# The head size must be a multiple of num_features.\n",
    "model_qat = TimeSeriesTransformer(\n",
    "    sequence_length=16, \n",
    "    num_features=3, \n",
    "    head_size=3,  # Each head will now have an embed size of 1 (3 / 3)\n",
    "    n_heads=1,  # Only one head since our embed_dim is 3\n",
    "    ff_dim=64, \n",
    "    n_trans_blocks=4, \n",
    "    mlp_units=[128, 64], \n",
    "    drop=0.1, \n",
    "    mlp_drop=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "772512c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": " qat.Linear.from_float only works for Linear",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_qat\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mget_default_qat_qconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqnnpack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model_qat\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m model_prepared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mprepare_qat(model_qat, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:492\u001b[0m, in \u001b[0;36mprepare_qat\u001b[0;34m(model, mapping, inplace)\u001b[0m\n\u001b[1;32m    489\u001b[0m     model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\n\u001b[1;32m    491\u001b[0m propagate_qconfig_(model, qconfig_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 492\u001b[0m convert(model, mapping\u001b[38;5;241m=\u001b[39mmapping, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_qconfig\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    493\u001b[0m prepare(model, observer_non_leaf_module_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(mapping\u001b[38;5;241m.\u001b[39mvalues()), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:550\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    549\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 550\u001b[0m _convert(\n\u001b[1;32m    551\u001b[0m     module, mapping, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_reference\u001b[38;5;241m=\u001b[39mis_reference,\n\u001b[1;32m    552\u001b[0m     convert_custom_config_dict\u001b[38;5;241m=\u001b[39mconvert_custom_config_dict)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    554\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:588\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    587\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 588\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    589\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m    590\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:588\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    587\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 588\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    589\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m    590\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:588\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    587\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 588\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    589\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m    590\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:590\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    587\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    588\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    589\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 590\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    593\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:623\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    621\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n\u001b[1;32m    624\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/ao/nn/qat/modules/linear.py:49\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_float\u001b[39m(\u001b[38;5;28mcls\u001b[39m, mod):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Create a qat module from a float module or qparams_dict\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m        Args: `mod` a float module, either produced by torch.ao.quantization utilities\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m        or directly from user\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_FLOAT_MODULE, (\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m qat.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.from_float only works for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_FLOAT_MODULE\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput float module must have qconfig defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mqconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput float module must have a valid qconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m:  qat.Linear.from_float only works for Linear"
     ]
    }
   ],
   "source": [
    "model_qat.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
    "model_qat.train()\n",
    "model_prepared = torch.quantization.prepare_qat(model_qat, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88070bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Instantiate the model\n",
    "pruned_model = TimeSeriesTransformer(\n",
    "    sequence_length=16, \n",
    "    num_features=3, \n",
    "    head_size=3,\n",
    "    n_heads=1, \n",
    "    ff_dim=64, \n",
    "    n_trans_blocks=4, \n",
    "    mlp_units=[128, 64], \n",
    "    drop=0.1, \n",
    "    mlp_drop=0.1\n",
    ")\n",
    "\n",
    "# Apply pruning to convolutional layers in each TransformerEncoderBlock\n",
    "for encoder_block in pruned_model.encoders:\n",
    "    prune.l1_unstructured(encoder_block.conv1, 'weight', amount=0.1)\n",
    "    prune.l1_unstructured(encoder_block.conv2, 'weight', amount=0.1)\n",
    "\n",
    "# Apply pruning to linear layers in the MLP\n",
    "for layer in pruned_model.mlp:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        prune.l1_unstructured(layer, 'weight', amount=0.1)\n",
    "\n",
    "# To make the pruning permanent\n",
    "for encoder_block in pruned_model.encoders:\n",
    "    prune.remove(encoder_block.conv1, 'weight')\n",
    "    prune.remove(encoder_block.conv2, 'weight')\n",
    "\n",
    "for layer in pruned_model.mlp:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        prune.remove(layer, 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29eb79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/Transformer_pruned.pth\"\n",
    "torch.save(pruned_model.state_dict(), pruned_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "942188b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 61.36 KB\n",
      "Accuracy on the test set: 22.25%\n",
      "CPU usage during inference: 34.55%\n",
      "Inference time: 1.4775 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, pruned_model, X_test_tensor, y_test_tensor, pruned_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334c01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
