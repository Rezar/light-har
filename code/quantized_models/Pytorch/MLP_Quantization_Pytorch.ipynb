{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e4447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bbb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x = genfromtxt('../Data/WISDM_x.csv', delimiter=',')\n",
    "y_df = pd.read_csv('../Data/WISDM_y.csv')\n",
    "y = y_df.values.flatten()  # Flatten if y is 2D\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Function to create time series dataset\n",
    "def create_series(x, y, timestep, overlap):\n",
    "    slide_step = int(timestep * (1 - overlap))\n",
    "    data_num = int((len(x) / slide_step) - 1)\n",
    "    dataset = np.ndarray(shape=(data_num, timestep, x.shape[1]))\n",
    "    labels = []\n",
    "\n",
    "    for i in range(data_num):\n",
    "        labels.append(y[slide_step * (i + 1) - 1])\n",
    "        for j in range(timestep):\n",
    "            dataset[i, j, :] = x[slide_step * i + j, :]\n",
    "\n",
    "    return dataset, np.array(labels)\n",
    "\n",
    "# Create time series\n",
    "timestep = 16  # Replace with your value\n",
    "overlap = 0.5  # Replace with your value\n",
    "X_series, y_series = create_series(x, y_encoded, timestep, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf3ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_series, y_series, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ab7d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:(109820, 16, 3), X_test.shape:(27455, 16, 3), y_train.shape:(109820,), y_test.shape:(27455,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train.shape:{X_train.shape}, X_test.shape:{X_test.shape}, y_train.shape:{y_train.shape}, y_test.shape:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3711d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Model Initialization\n",
    "input_size = timestep * X_series.shape[2]  # Calculate input size\n",
    "model = MyMLP(input_size)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fed4e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.501120697316173\n",
      "Epoch 1, Loss: 0.36820678322604206\n",
      "Epoch 2, Loss: 0.3228094878417629\n",
      "Epoch 3, Loss: 0.29296664734281996\n",
      "Epoch 4, Loss: 0.27289692796158904\n",
      "Epoch 5, Loss: 0.25624155666731246\n",
      "Epoch 6, Loss: 0.2421755583409767\n",
      "Epoch 7, Loss: 0.2312176041869503\n",
      "Epoch 8, Loss: 0.22040794937847516\n",
      "Epoch 9, Loss: 0.21252446349206575\n",
      "Epoch 10, Loss: 0.2048054636979263\n",
      "Epoch 11, Loss: 0.20062535022991104\n",
      "Epoch 12, Loss: 0.19355639066907526\n",
      "Epoch 13, Loss: 0.18681977839059347\n",
      "Epoch 14, Loss: 0.18382549256472772\n",
      "Epoch 15, Loss: 0.17833498907915918\n",
      "Epoch 16, Loss: 0.17454950653650475\n",
      "Epoch 17, Loss: 0.1704424264393573\n",
      "Epoch 18, Loss: 0.16721144855339384\n",
      "Epoch 19, Loss: 0.163832346694913\n",
      "Epoch 20, Loss: 0.16014591002313633\n",
      "Epoch 21, Loss: 0.15955315573067166\n",
      "Epoch 22, Loss: 0.15480938162068492\n",
      "Epoch 23, Loss: 0.15342402762010485\n",
      "Epoch 24, Loss: 0.15050017489402584\n",
      "Epoch 25, Loss: 0.14863521820162195\n",
      "Epoch 26, Loss: 0.14575443329671636\n",
      "Epoch 27, Loss: 0.14462723103102235\n",
      "Epoch 28, Loss: 0.14224092444483113\n",
      "Epoch 29, Loss: 0.13944873842682107\n",
      "Epoch 30, Loss: 0.13886885959893847\n",
      "Epoch 31, Loss: 0.1373920513535166\n",
      "Epoch 32, Loss: 0.13458604545121913\n",
      "Epoch 33, Loss: 0.13451757514035212\n",
      "Epoch 34, Loss: 0.13122557109712443\n",
      "Epoch 35, Loss: 0.1290928301241874\n",
      "Epoch 36, Loss: 0.1270332028025583\n",
      "Epoch 37, Loss: 0.12864406307397241\n",
      "Epoch 38, Loss: 0.1271205651861483\n",
      "Epoch 39, Loss: 0.12548269792937505\n",
      "Epoch 40, Loss: 0.12401679274571803\n",
      "Epoch 41, Loss: 0.12230984006719642\n",
      "Epoch 42, Loss: 0.1209412638058925\n",
      "Epoch 43, Loss: 0.1212668007452449\n",
      "Epoch 44, Loss: 0.12034162262667379\n",
      "Epoch 45, Loss: 0.11722240670386588\n",
      "Epoch 46, Loss: 0.11611596971374691\n",
      "Epoch 47, Loss: 0.11642629148966105\n",
      "Epoch 48, Loss: 0.11531645025366521\n",
      "Epoch 49, Loss: 0.1135659453791234\n",
      "Epoch 50, Loss: 0.1151942018281651\n",
      "Epoch 51, Loss: 0.11122325131515505\n",
      "Epoch 52, Loss: 0.11042352709056276\n",
      "Epoch 53, Loss: 0.11241831381572953\n",
      "Epoch 54, Loss: 0.11003457346175621\n",
      "Epoch 55, Loss: 0.10901292873110127\n",
      "Epoch 56, Loss: 0.10666255837405157\n",
      "Epoch 57, Loss: 0.10768121508132485\n",
      "Epoch 58, Loss: 0.10736084127750374\n",
      "Epoch 59, Loss: 0.10624995364374448\n",
      "Epoch 60, Loss: 0.1038725565199573\n",
      "Epoch 61, Loss: 0.10506425196650093\n",
      "Epoch 62, Loss: 0.10600243487812162\n",
      "Epoch 63, Loss: 0.10322019257278796\n",
      "Epoch 64, Loss: 0.10196890096472672\n",
      "Epoch 65, Loss: 0.10176563474088082\n",
      "Epoch 66, Loss: 0.10233936503153437\n",
      "Epoch 67, Loss: 0.10146543946923657\n",
      "Epoch 68, Loss: 0.10065206852855513\n",
      "Epoch 69, Loss: 0.09727198198016533\n",
      "Epoch 70, Loss: 0.09995965306464764\n",
      "Epoch 71, Loss: 0.09764272122267234\n",
      "Epoch 72, Loss: 0.09861171022852631\n",
      "Epoch 73, Loss: 0.09606622623487013\n",
      "Epoch 74, Loss: 0.09838505261897876\n",
      "Epoch 75, Loss: 0.09648664432332475\n",
      "Epoch 76, Loss: 0.0956795098525168\n",
      "Epoch 77, Loss: 0.09530112394788422\n",
      "Epoch 78, Loss: 0.09438868615470201\n",
      "Epoch 79, Loss: 0.0951191487549511\n",
      "Epoch 80, Loss: 0.09396154057079305\n",
      "Epoch 81, Loss: 0.09357755334213667\n",
      "Epoch 82, Loss: 0.09149210413484801\n",
      "Epoch 83, Loss: 0.09370297152415517\n",
      "Epoch 84, Loss: 0.0936419813110035\n",
      "Epoch 85, Loss: 0.09144600492165074\n",
      "Epoch 86, Loss: 0.09093065831526932\n",
      "Epoch 87, Loss: 0.0897486602596066\n",
      "Epoch 88, Loss: 0.09075812808599427\n",
      "Epoch 89, Loss: 0.08998429938389091\n",
      "Epoch 90, Loss: 0.08829630918755106\n",
      "Epoch 91, Loss: 0.08915452212759425\n",
      "Epoch 92, Loss: 0.08788152537316775\n",
      "Epoch 93, Loss: 0.08896820749509493\n",
      "Epoch 94, Loss: 0.08865579869543877\n",
      "Epoch 95, Loss: 0.08565622585497813\n",
      "Epoch 96, Loss: 0.08763532548907293\n",
      "Epoch 97, Loss: 0.08861136099373561\n",
      "Epoch 98, Loss: 0.08513647405279254\n",
      "Epoch 99, Loss: 0.08660408965228364\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "#\n",
    "model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_base.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74ae418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.45919706777285235, Accuracy: 25085/27455 (91%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test set: Average loss: {total_loss / len(test_loader)}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)')\n",
    "\n",
    "# DataLoader for test set\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a77b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=48, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6776a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_metrics_base(model, x_test, y_test, model_path):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the PyTorch model.\n",
    "\n",
    "    :param model: PyTorch model.\n",
    "    :param x_test: Test dataset features (as a PyTorch Tensor).\n",
    "    :param y_test: Test dataset labels (as a NumPy array).\n",
    "    :param model_dir: Directory where the PyTorch model files are stored.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the model's predictions\n",
    "        outputs = model(x_test)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "        # Convert y_test to tensor if it's not already\n",
    "        true_labels = torch.tensor(y_test) if not isinstance(y_test, torch.Tensor) else y_test\n",
    "        true_labels = true_labels.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "    model_file = Path(model_path)\n",
    "\n",
    "    # Size in bytes\n",
    "    model_size_bytes = model_file.stat().st_size\n",
    "\n",
    "    # Convert size to kilobytes (optional)\n",
    "    model_size_kb = model_size_bytes / 1024\n",
    "    print(f\"Size of the model: {model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(true_labels.numpy(), predicted_labels.numpy())\n",
    "    print(f'Accuracy on the test set: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a7b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_cpu_utilization_and_run(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measure CPU utilization while running a function.\n",
    "\n",
    "    Parameters:\n",
    "        func (function): The function to be executed.\n",
    "        *args: Arguments to be passed to func.\n",
    "        **kwargs: Keyword arguments to be passed to func.\n",
    "\n",
    "    Returns:\n",
    "        float: CPU utilization percentage during the execution of func.\n",
    "        float: The elapsed time during the execution of func.\n",
    "        any: The result of func execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Measure CPU utilization before execution\n",
    "    cpu_percent_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Execute the function and store its result\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Measure CPU utilization after execution\n",
    "    cpu_percent_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # Calculate elapsed time and average CPU utilization\n",
    "    elapsed_time = end_time - start_time\n",
    "    average_cpu_utilization = (cpu_percent_before + cpu_percent_after) / 2\n",
    "\n",
    "    return average_cpu_utilization, elapsed_time, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c017afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 60.83 KB\n",
      "Accuracy on the test set: 91.37%\n",
      "CPU usage during inference: 28.90%\n",
      "Inference time: 0.0090 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model, x_test_tensor, y_test_tensor, model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b09d0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    {nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9abb15a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Quantized.pth\"\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "337b00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 19.58 KB\n",
      "Accuracy on the test set: 91.26%\n",
      "CPU usage during inference: 39.40%\n",
      "Inference time: 0.0219 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, quantized_model, x_test_tensor, y_test_tensor, quantized_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc0fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_predictions(model, x_test, y_test, num_samples=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Predict on the test set\n",
    "        outputs = model(x_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        print(\"Sample predictions:\\n\")\n",
    "        for i in range(num_samples):\n",
    "            print(f\"x_test[{i}]: {x_test[i]}\")\n",
    "            print(f\"Actual label (y_test[{i}]): {y_test[i]}\")\n",
    "            print(f\"Predicted label: {predicted[i]}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448a9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you're using the first num_samples of x_test and y_test\n",
    "#num_samples = 5\n",
    "#print_sample_predictions(model, x_test_tensor[:num_samples], y_test_tensor[:num_samples], num_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fd350",
   "metadata": {},
   "source": [
    "### Static Quantization - Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c74801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.8709326718514894, Accuracy: 3635/27455 (13%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_fp32 = MyMLP(input_size)\n",
    "model_fp32.load_state_dict(torch.load(model_path))\n",
    "model_fp32.eval()\n",
    "\n",
    "# Define a quantization configuration\n",
    "model_int8 = QuantizedMLP(input_size)\n",
    "model_int8.eval()\n",
    "\n",
    "# Specify the quantization configuration\n",
    "model_int8.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "torch.quantization.prepare(model_int8, inplace=True)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "# Assuming the train_loader is representative of the data distribution\n",
    "for data, _ in train_loader:\n",
    "    model_int8(data)\n",
    "\n",
    "# Convert to a quantized model\n",
    "torch.quantization.convert(model_int8, inplace=True)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "evaluate(model_int8, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf9554a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to /Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Static_Quantized.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the quantized model\n",
    "static_quantized_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Static_Quantized.pth\"\n",
    "\n",
    "# Save the state dictionary of the quantized model\n",
    "torch.save(model_int8.state_dict(), static_quantized_model_path)\n",
    "\n",
    "print(f\"Quantized model saved to {static_quantized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a10ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 20.27 KB\n",
      "Accuracy on the test set: 13.24%\n",
      "CPU usage during inference: 49.80%\n",
      "Inference time: 0.0193 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model_int8, x_test_tensor, y_test_tensor, static_quantized_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912faeba",
   "metadata": {},
   "source": [
    "### Static Quantization - Per Channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a004b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.6373933064631927, Accuracy: 8338/27455 (30%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub, default_per_channel_qconfig\n",
    "\n",
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_fp32 = MyMLP(input_size)\n",
    "model_fp32.load_state_dict(torch.load(model_path))\n",
    "model_fp32.eval()\n",
    "\n",
    "# Define a quantization configuration\n",
    "model_int8_pc = QuantizedMLP(input_size)\n",
    "model_int8_pc.eval()\n",
    "\n",
    "# Specify the quantization configuration to use per-channel weight quantization\n",
    "model_int8_pc.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "# Set the model configuration to use per-channel quantization\n",
    "model_int8_pc.fc1.qconfig = default_per_channel_qconfig\n",
    "model_int8_pc.fc2.qconfig = default_per_channel_qconfig\n",
    "# For the output layer, you might want to use per-tensor quantization\n",
    "model_int8_pc.fc3.qconfig = torch.quantization.default_qconfig\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "torch.quantization.prepare(model_int8_pc, inplace=True)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "# Assuming the train_loader is representative of the data distribution\n",
    "for data, _ in train_loader:\n",
    "    model_int8_pc(data)\n",
    "\n",
    "# Convert to a quantized model\n",
    "torch.quantization.convert(model_int8_pc, inplace=True)\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Static_Quantized_perChannel.pth\"\n",
    "torch.save(model_int8_pc.state_dict(), quantized_model_path)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "evaluate(model_int8_pc, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c70c2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 24.61 KB\n",
      "Accuracy on the test set: 30.37%\n",
      "CPU usage during inference: 38.90%\n",
      "Inference time: 0.0053 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model_int8_pc, x_test_tensor, y_test_tensor, quantized_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df9cdb",
   "metadata": {},
   "source": [
    "### Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9446c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the model architecture for QAT\n",
    "class MyMLPForQAT(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(MyMLPForQAT, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b9e3e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeep/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.20328503847122192\n",
      "Epoch 1: Loss 0.33764880895614624\n",
      "Epoch 2: Loss 0.2922326624393463\n",
      "Epoch 3: Loss 0.3087995946407318\n",
      "Epoch 4: Loss 0.4398750364780426\n",
      "Epoch 5: Loss 0.327178418636322\n",
      "Epoch 6: Loss 0.34778526425361633\n",
      "Epoch 7: Loss 0.14742764830589294\n",
      "Epoch 8: Loss 0.5550677180290222\n",
      "Epoch 9: Loss 0.25033774971961975\n"
     ]
    }
   ],
   "source": [
    "# Assuming the correct input size and number of classes\n",
    "input_size = 16 * 3  # 16 time steps with 3 features each\n",
    "num_classes = 6  # Assuming 6 classes as per your data\n",
    "\n",
    "# Instantiate and prepare the model for QAT\n",
    "model_qat = MyMLPForQAT(input_size, num_classes)\n",
    "model_qat.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "model_prepared = torch.quantization.prepare_qat(model_qat, inplace=True)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model_qat.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning loop for QAT\n",
    "num_fine_tune_epochs = 10\n",
    "model_prepared.train()\n",
    "for epoch in range(num_fine_tune_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), -1)  # Flatten the input\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_prepared(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}: Loss {loss.item()}')\n",
    "\n",
    "model_prepared.eval()\n",
    "# Convert the QAT model to a fully quantized model\n",
    "qat_model = torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# Save the fine-tuned quantized model\n",
    "qat_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_QAT_v2.pth\"\n",
    "torch.save(qat_model.state_dict(), qat_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39e474eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.37%\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model for evaluation\n",
    "qat_model.eval()\n",
    "\n",
    "# Define the test dataset and dataloader\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.view(inputs.size(0), -1)  # Flatten the input\n",
    "        outputs = qat_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9dc99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_metrics_new(model, x_test, y_test, model_path):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the PyTorch model.\n",
    "\n",
    "    :param model: PyTorch model.\n",
    "    :param x_test: Test dataset features (as a PyTorch Tensor).\n",
    "    :param y_test: Test dataset labels (as a NumPy array).\n",
    "    :param model_dir: Directory where the PyTorch model files are stored.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Flatten the input\n",
    "            outputs = qat_model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "    model_file = Path(model_path)\n",
    "    # Size in bytes\n",
    "    model_size_bytes = model_file.stat().st_size\n",
    "\n",
    "    # Convert size to kilobytes (optional)\n",
    "    model_size_kb = model_size_bytes / 1024\n",
    "    print(f\"Size of the model: {model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78a96d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 60.85 KB\n",
      "Accuracy on the test set: 91.37%\n",
      "CPU usage during inference: 26.60%\n",
      "Inference time: 0.1265 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_new, qat_model, x_test_tensor, y_test_tensor, qat_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
