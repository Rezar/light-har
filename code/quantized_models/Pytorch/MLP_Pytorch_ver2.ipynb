{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e4447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bbb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x = genfromtxt('../Data/WISDM_x.csv', delimiter=',')\n",
    "y_df = pd.read_csv('../Data/WISDM_y.csv')\n",
    "y = y_df.values.flatten()  # Flatten if y is 2D\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Function to create time series dataset\n",
    "def create_series(x, y, timestep, overlap):\n",
    "    slide_step = int(timestep * (1 - overlap))\n",
    "    data_num = int((len(x) / slide_step) - 1)\n",
    "    dataset = np.ndarray(shape=(data_num, timestep, x.shape[1]))\n",
    "    labels = []\n",
    "\n",
    "    for i in range(data_num):\n",
    "        labels.append(y[slide_step * (i + 1) - 1])\n",
    "        for j in range(timestep):\n",
    "            dataset[i, j, :] = x[slide_step * i + j, :]\n",
    "\n",
    "    return dataset, np.array(labels)\n",
    "\n",
    "# Create time series\n",
    "timestep = 16  # Replace with your value\n",
    "overlap = 0.5  # Replace with your value\n",
    "X_series, y_series = create_series(x, y_encoded, timestep, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf3ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_series, y_series, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ab7d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:(109820, 16, 3), X_test.shape:(27455, 16, 3), y_train.shape:(109820,), y_test.shape:(27455,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train.shape:{X_train.shape}, X_test.shape:{X_test.shape}, y_train.shape:{y_train.shape}, y_test.shape:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3711d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Model Initialization\n",
    "input_size = timestep * X_series.shape[2]  # Calculate input size\n",
    "model = MyMLP(input_size)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fed4e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5036703805875528\n",
      "Epoch 1, Loss: 0.3736385640694753\n",
      "Epoch 2, Loss: 0.3213703603264474\n",
      "Epoch 3, Loss: 0.2899197906663854\n",
      "Epoch 4, Loss: 0.26723053082954007\n",
      "Epoch 5, Loss: 0.25139076860582493\n",
      "Epoch 6, Loss: 0.23830647413599323\n",
      "Epoch 7, Loss: 0.22708436165282847\n",
      "Epoch 8, Loss: 0.21918042258322587\n",
      "Epoch 9, Loss: 0.21003089617234427\n",
      "Epoch 10, Loss: 0.20360947032795573\n",
      "Epoch 11, Loss: 0.19722458829404244\n",
      "Epoch 12, Loss: 0.19203124335361318\n",
      "Epoch 13, Loss: 0.18720296161718197\n",
      "Epoch 14, Loss: 0.18219791885925618\n",
      "Epoch 15, Loss: 0.17841651994031626\n",
      "Epoch 16, Loss: 0.17403507972622426\n",
      "Epoch 17, Loss: 0.16986562935895957\n",
      "Epoch 18, Loss: 0.16700676376095566\n",
      "Epoch 19, Loss: 0.16328140604689648\n",
      "Epoch 20, Loss: 0.1612280957696043\n",
      "Epoch 21, Loss: 0.15745123630142085\n",
      "Epoch 22, Loss: 0.1557973129609665\n",
      "Epoch 23, Loss: 0.1525420074173117\n",
      "Epoch 24, Loss: 0.15005502231106474\n",
      "Epoch 25, Loss: 0.14734081920813813\n",
      "Epoch 26, Loss: 0.14468327838284917\n",
      "Epoch 27, Loss: 0.1431623719445927\n",
      "Epoch 28, Loss: 0.14156859976267724\n",
      "Epoch 29, Loss: 0.1372506077795361\n",
      "Epoch 30, Loss: 0.1379840317006399\n",
      "Epoch 31, Loss: 0.13568764261137217\n",
      "Epoch 32, Loss: 0.1332451510492773\n",
      "Epoch 33, Loss: 0.13267016341575447\n",
      "Epoch 34, Loss: 0.13063653397519306\n",
      "Epoch 35, Loss: 0.12886874874432883\n",
      "Epoch 36, Loss: 0.12825108362993526\n",
      "Epoch 37, Loss: 0.12689692947697676\n",
      "Epoch 38, Loss: 0.12509184626706102\n",
      "Epoch 39, Loss: 0.1251465939142739\n",
      "Epoch 40, Loss: 0.12194927213691818\n",
      "Epoch 41, Loss: 0.12322440349913717\n",
      "Epoch 42, Loss: 0.12171704732402638\n",
      "Epoch 43, Loss: 0.12089300431400138\n",
      "Epoch 44, Loss: 0.1168109966023545\n",
      "Epoch 45, Loss: 0.11736940837732249\n",
      "Epoch 46, Loss: 0.11774300902024658\n",
      "Epoch 47, Loss: 0.1161740866602596\n",
      "Epoch 48, Loss: 0.11530686874786014\n",
      "Epoch 49, Loss: 0.11354550833641258\n",
      "Epoch 50, Loss: 0.11249417767236942\n",
      "Epoch 51, Loss: 0.11220653529608914\n",
      "Epoch 52, Loss: 0.11098411271732847\n",
      "Epoch 53, Loss: 0.11000872056199659\n",
      "Epoch 54, Loss: 0.11003543105381292\n",
      "Epoch 55, Loss: 0.10888318223352295\n",
      "Epoch 56, Loss: 0.10716669767224479\n",
      "Epoch 57, Loss: 0.1082814248432872\n",
      "Epoch 58, Loss: 0.10619419665561038\n",
      "Epoch 59, Loss: 0.1074379632734674\n",
      "Epoch 60, Loss: 0.10467286819607315\n",
      "Epoch 61, Loss: 0.1045157025692472\n",
      "Epoch 62, Loss: 0.10553112575154218\n",
      "Epoch 63, Loss: 0.1020295715848492\n",
      "Epoch 64, Loss: 0.10187193153953583\n",
      "Epoch 65, Loss: 0.10314179252858094\n",
      "Epoch 66, Loss: 0.10082864133293234\n",
      "Epoch 67, Loss: 0.10035742549871535\n",
      "Epoch 68, Loss: 0.10206863769554773\n",
      "Epoch 69, Loss: 0.09908309724835336\n",
      "Epoch 70, Loss: 0.0987733822076588\n",
      "Epoch 71, Loss: 0.09859243689793094\n",
      "Epoch 72, Loss: 0.09722019820544643\n",
      "Epoch 73, Loss: 0.09787532192408509\n",
      "Epoch 74, Loss: 0.09700087440500822\n",
      "Epoch 75, Loss: 0.09672029841623957\n",
      "Epoch 76, Loss: 0.09676909833695492\n",
      "Epoch 77, Loss: 0.09379703969703358\n",
      "Epoch 78, Loss: 0.09527041970226818\n",
      "Epoch 79, Loss: 0.09670334181838966\n",
      "Epoch 80, Loss: 0.09304435737132548\n",
      "Epoch 81, Loss: 0.09400386943241433\n",
      "Epoch 82, Loss: 0.09384256135102466\n",
      "Epoch 83, Loss: 0.09126334849881802\n",
      "Epoch 84, Loss: 0.09313098653173509\n",
      "Epoch 85, Loss: 0.09050156502324506\n",
      "Epoch 86, Loss: 0.09109972600430567\n",
      "Epoch 87, Loss: 0.09139925448711318\n",
      "Epoch 88, Loss: 0.08993793501315686\n",
      "Epoch 89, Loss: 0.08894791894297643\n",
      "Epoch 90, Loss: 0.09068621498058248\n",
      "Epoch 91, Loss: 0.08933350243749497\n",
      "Epoch 92, Loss: 0.08917041389433893\n",
      "Epoch 93, Loss: 0.08912618158725448\n",
      "Epoch 94, Loss: 0.08750880122500079\n",
      "Epoch 95, Loss: 0.09008490069894694\n",
      "Epoch 96, Loss: 0.08675573166380561\n",
      "Epoch 97, Loss: 0.08601662122301479\n",
      "Epoch 98, Loss: 0.08819916728888866\n",
      "Epoch 99, Loss: 0.08526265237150422\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "#\n",
    "model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_base.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ae418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4956688749015609, Accuracy: 25021/27455 (91%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test set: Average loss: {total_loss / len(test_loader)}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)')\n",
    "\n",
    "# DataLoader for test set\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbcbb052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=48, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6776a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_metrics_base(model, x_test, y_test, model_path):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the PyTorch model.\n",
    "\n",
    "    :param model: PyTorch model.\n",
    "    :param x_test: Test dataset features (as a PyTorch Tensor).\n",
    "    :param y_test: Test dataset labels (as a NumPy array).\n",
    "    :param model_dir: Directory where the PyTorch model files are stored.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the model's predictions\n",
    "        outputs = model(x_test)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "        # Convert y_test to tensor if it's not already\n",
    "        true_labels = torch.tensor(y_test) if not isinstance(y_test, torch.Tensor) else y_test\n",
    "        true_labels = true_labels.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "    model_file = Path(model_path)\n",
    "\n",
    "    # Size in bytes\n",
    "    model_size_bytes = model_file.stat().st_size\n",
    "\n",
    "    # Convert size to kilobytes (optional)\n",
    "    model_size_kb = model_size_bytes / 1024\n",
    "    print(f\"Size of the model: {model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(true_labels.numpy(), predicted_labels.numpy())\n",
    "    print(f'Accuracy on the test set: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a7b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_cpu_utilization_and_run(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measure CPU utilization while running a function.\n",
    "\n",
    "    Parameters:\n",
    "        func (function): The function to be executed.\n",
    "        *args: Arguments to be passed to func.\n",
    "        **kwargs: Keyword arguments to be passed to func.\n",
    "\n",
    "    Returns:\n",
    "        float: CPU utilization percentage during the execution of func.\n",
    "        float: The elapsed time during the execution of func.\n",
    "        any: The result of func execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Measure CPU utilization before execution\n",
    "    cpu_percent_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Execute the function and store its result\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Measure CPU utilization after execution\n",
    "    cpu_percent_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # Calculate elapsed time and average CPU utilization\n",
    "    elapsed_time = end_time - start_time\n",
    "    average_cpu_utilization = (cpu_percent_before + cpu_percent_after) / 2\n",
    "\n",
    "    return average_cpu_utilization, elapsed_time, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c017afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 60.83 KB\n",
      "Accuracy on the test set: 91.13%\n",
      "CPU usage during inference: 32.50%\n",
      "Inference time: 0.0084 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model, x_test_tensor, y_test_tensor, model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b09d0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    {nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abb15a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Quantized.pth\"\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "337b00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 19.58 KB\n",
      "Accuracy on the test set: 91.08%\n",
      "CPU usage during inference: 40.55%\n",
      "Inference time: 0.0298 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, quantized_model, x_test_tensor, y_test_tensor, quantized_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dc0fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_predictions(model, x_test, y_test, num_samples=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Predict on the test set\n",
    "        outputs = model(x_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        print(\"Sample predictions:\\n\")\n",
    "        for i in range(num_samples):\n",
    "            print(f\"x_test[{i}]: {x_test[i]}\")\n",
    "            print(f\"Actual label (y_test[{i}]): {y_test[i]}\")\n",
    "            print(f\"Predicted label: {predicted[i]}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "448a9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you're using the first num_samples of x_test and y_test\n",
    "#num_samples = 5\n",
    "#print_sample_predictions(model, x_test_tensor[:num_samples], y_test_tensor[:num_samples], num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3828997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.2.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "XGBoost version 1.7.6 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
      "TensorFlow version 2.15.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coremltools.converters.pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcoremltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coremltools.converters.pytorch'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458b253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a1fd350",
   "metadata": {},
   "source": [
    "### Static Quantization - Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c74801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.326750990394112, Accuracy: 3509/27455 (13%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_fp32 = MyMLP(input_size)\n",
    "model_fp32.load_state_dict(torch.load(model_path))\n",
    "model_fp32.eval()\n",
    "\n",
    "# Define a quantization configuration\n",
    "model_int8 = QuantizedMLP(input_size)\n",
    "model_int8.eval()\n",
    "\n",
    "# Specify the quantization configuration\n",
    "model_int8.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "torch.quantization.prepare(model_int8, inplace=True)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "# Assuming the train_loader is representative of the data distribution\n",
    "for data, _ in train_loader:\n",
    "    model_int8(data)\n",
    "\n",
    "# Convert to a quantized model\n",
    "torch.quantization.convert(model_int8, inplace=True)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "evaluate(model_int8, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf9554a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to /Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Static_Quantized.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the quantized model\n",
    "static_quantized_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Static_Quantized.pth\"\n",
    "\n",
    "# Save the state dictionary of the quantized model\n",
    "torch.save(model_int8.state_dict(), static_quantized_model_path)\n",
    "\n",
    "print(f\"Quantized model saved to {static_quantized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a10ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 20.27 KB\n",
      "Accuracy on the test set: 12.78%\n",
      "CPU usage during inference: 46.55%\n",
      "Inference time: 0.0049 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model_int8, x_test_tensor, y_test_tensor, static_quantized_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c3b32",
   "metadata": {},
   "source": [
    "### Static Quantization - Per Channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a004b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.8900043008488654, Accuracy: 10407/27455 (38%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub, default_per_channel_qconfig\n",
    "\n",
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_fp32 = MyMLP(input_size)\n",
    "model_fp32.load_state_dict(torch.load(model_path))\n",
    "model_fp32.eval()\n",
    "\n",
    "# Define a quantization configuration\n",
    "model_int8_pc = QuantizedMLP(input_size)\n",
    "model_int8_pc.eval()\n",
    "\n",
    "# Specify the quantization configuration to use per-channel weight quantization\n",
    "model_int8_pc.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "# Set the model configuration to use per-channel quantization\n",
    "model_int8_pc.fc1.qconfig = default_per_channel_qconfig\n",
    "model_int8_pc.fc2.qconfig = default_per_channel_qconfig\n",
    "# For the output layer, you might want to use per-tensor quantization\n",
    "model_int8_pc.fc3.qconfig = torch.quantization.default_qconfig\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "torch.quantization.prepare(model_int8_pc, inplace=True)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "# Assuming the train_loader is representative of the data distribution\n",
    "for data, _ in train_loader:\n",
    "    model_int8_pc(data)\n",
    "\n",
    "# Convert to a quantized model\n",
    "torch.quantization.convert(model_int8_pc, inplace=True)\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_Static_Quantized_perChannel.pth\"\n",
    "torch.save(model_int8_pc.state_dict(), quantized_model_path)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "evaluate(model_int8_pc, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c70c2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 24.61 KB\n",
      "Accuracy on the test set: 37.91%\n",
      "CPU usage during inference: 36.15%\n",
      "Inference time: 0.0051 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model_int8_pc, x_test_tensor, y_test_tensor, quantized_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df9cdb",
   "metadata": {},
   "source": [
    "### Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9446c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the model architecture for QAT\n",
    "class MyMLPForQAT(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(MyMLPForQAT, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "188bf068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.0491827726364136\n",
      "Epoch 1: Loss 0.8117403388023376\n",
      "Epoch 2: Loss 0.9130408763885498\n",
      "Epoch 3: Loss 0.7163098454475403\n",
      "Epoch 4: Loss 0.6618160605430603\n",
      "Epoch 5: Loss 0.6053312420845032\n",
      "Epoch 6: Loss 0.42409414052963257\n",
      "Epoch 7: Loss 0.5860022902488708\n",
      "Epoch 8: Loss 0.6115912199020386\n",
      "Epoch 9: Loss 0.6875667572021484\n"
     ]
    }
   ],
   "source": [
    "# Assuming the correct input size and number of classes\n",
    "input_size = 16 * 3  # 16 time steps with 3 features each\n",
    "num_classes = 6  # Assuming 6 classes as per your data\n",
    "\n",
    "# Instantiate and prepare the model for QAT\n",
    "model_qat = MyMLPForQAT(input_size, num_classes)\n",
    "model_qat.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
    "model_qat.train()\n",
    "model_prepared = torch.quantization.prepare_qat(model_qat, inplace=True)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model_prepared.parameters(), lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning loop for QAT\n",
    "num_fine_tune_epochs = 10\n",
    "model_prepared.train()\n",
    "for epoch in range(num_fine_tune_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), -1)  # Flatten the input\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_prepared(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}: Loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d320b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prepared.eval()\n",
    "# Convert the QAT model to a fully quantized model\n",
    "qat_model = torch.quantization.convert(model, inplace=False)\n",
    "\n",
    "# Save the fine-tuned quantized model\n",
    "qat_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_QAT_v2.pth\"\n",
    "torch.save(qat_model.state_dict(), qat_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f93fb9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLPForQAT(\n",
       "  (quant): QuantStub()\n",
       "  (fc1): Linear(in_features=48, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the QAT model from the saved file\n",
    "qat_model_saved = MyMLPForQAT(input_size, num_classes)\n",
    "state_dict = torch.load(model_path)\n",
    "qat_model_saved.load_state_dict(state_dict)\n",
    "qat_model_saved.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4ac8e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.92%\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model for evaluation\n",
    "qat_model_saved.eval()\n",
    "\n",
    "# Define the test dataset and dataloader\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.view(inputs.size(0), -1)  # Flatten the input\n",
    "        outputs = qat_model_saved(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ff0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_metrics_new(model, x_test, y_test, model_path):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the PyTorch model.\n",
    "\n",
    "    :param model: PyTorch model.\n",
    "    :param x_test: Test dataset features (as a PyTorch Tensor).\n",
    "    :param y_test: Test dataset labels (as a NumPy array).\n",
    "    :param model_dir: Directory where the PyTorch model files are stored.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Flatten the input\n",
    "            outputs = qat_model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "    model_file = Path(model_path)\n",
    "    # Size in bytes\n",
    "    model_size_bytes = model_file.stat().st_size\n",
    "\n",
    "    # Convert size to kilobytes (optional)\n",
    "    model_size_kb = model_size_bytes / 1024\n",
    "    print(f\"Size of the model: {model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a234ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 60.85 KB\n",
      "Accuracy on the test set: 90.92%\n",
      "CPU usage during inference: 21.85%\n",
      "Inference time: 0.0879 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure CPU usage and inference time\n",
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_new, qat_model_saved, x_test_tensor, y_test_tensor, qat_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8b26b",
   "metadata": {},
   "source": [
    "### Torch Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c48bc177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=48, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "288f4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "# Assuming timestep and X_series.shape[2] are defined\n",
    "# input_size = timestep * X_series.shape[2]\n",
    "pruned_model = MyMLP(input_size)\n",
    "\n",
    "# Apply pruning to a layer by specifying the percentage of connections to prune\n",
    "prune.l1_unstructured(pruned_model.fc1, 'weight', amount=0.2)\n",
    "prune.l1_unstructured(pruned_model.fc2, 'weight', amount=0.2)\n",
    "\n",
    "# To make the pruning permanent, you might want to remove the reparametrization\n",
    "for module in [pruned_model.fc1, pruned_model.fc2]:\n",
    "    prune.remove(module, 'weight')\n",
    "\n",
    "\n",
    "pruned_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_pruned.pth\"\n",
    "torch.save(model.state_dict(), pruned_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dabc3638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.0504767550177228, Accuracy: 2677/27455 (10%)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "evaluate(pruned_model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9176afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 60.85 KB\n",
      "Accuracy on the test set: 9.75%\n",
      "CPU usage during inference: 12.80%\n",
      "Inference time: 0.0083 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, pruned_model, x_test_tensor, y_test_tensor, pruned_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07e498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3894c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = MyMLP(input_size)\n",
    "\n",
    "# Define the amount of pruning\n",
    "pruning_amount = 0.2  # This will prune 20% of the neurons\n",
    "\n",
    "# Apply structured pruning to the layers\n",
    "prune.ln_structured(model.fc1, name=\"weight\", amount=pruning_amount, n=1, dim=0)\n",
    "prune.ln_structured(model.fc2, name=\"weight\", amount=pruning_amount, n=1, dim=0)\n",
    "\n",
    "# Optionally, make the pruning permanent\n",
    "for module in [model.fc1, model.fc2]:\n",
    "    prune.remove(module, 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c726e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_pruned_structured.pth\"\n",
    "torch.save(model.state_dict(), pruned_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96d43ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.0075885318653843, Accuracy: 2826/27455 (10%)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca830fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 60.96 KB\n",
      "Accuracy on the test set: 10.29%\n",
      "CPU usage during inference: 0.00%\n",
      "Inference time: 0.0064 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model, x_test_tensor, y_test_tensor, pruned_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd2486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17e108f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5167649119727679\n",
      "Accuracy of the model on the validation set: 84.09032963030414%\n",
      "Epoch [2/10], Loss: 0.3852498353543607\n",
      "Accuracy of the model on the validation set: 87.40848661446003%\n",
      "Epoch [3/10], Loss: 0.3363587231807289\n",
      "Accuracy of the model on the validation set: 88.72336550719359%\n",
      "Epoch [4/10], Loss: 0.30365206413231527\n",
      "Accuracy of the model on the validation set: 89.12037880167547%\n",
      "Epoch [5/10], Loss: 0.2822448457949437\n",
      "Accuracy of the model on the validation set: 89.4773265343289%\n",
      "Epoch [6/10], Loss: 0.26496579775061363\n",
      "Accuracy of the model on the validation set: 89.63030413403752%\n",
      "Epoch [7/10], Loss: 0.25294134031877025\n",
      "Accuracy of the model on the validation set: 90.38790748497541%\n",
      "Epoch [8/10], Loss: 0.2416109860047594\n",
      "Accuracy of the model on the validation set: 90.2094336186487%\n",
      "Epoch [9/10], Loss: 0.2334713713962819\n",
      "Accuracy of the model on the validation set: 90.42433072300128%\n",
      "Epoch [10/10], Loss: 0.22536255340884268\n",
      "Accuracy of the model on the validation set: 90.97067929338918%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Use a lower learning rate for fine-tuning\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 10  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy of the model on the validation set: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33de0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_pruned_structured_finetuned.pth\"\n",
    "torch.save(model.state_dict(), pruned_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd9f1b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 61.12 KB\n",
      "Accuracy on the test set: 90.97%\n",
      "CPU usage during inference: 12.35%\n",
      "Inference time: 0.0073 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, model, x_test_tensor, y_test_tensor, pruned_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdf46f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        # Initialize a dictionary to hold the sum of activations\n",
    "        self.activations = {'fc1': 0, 'fc2': 0}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Sum the absolute values of the activations for fc1\n",
    "        self.activations['fc1'] += x.abs().sum(dim=0)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Sum the absolute values of the activations for fc2\n",
    "        self.activations['fc2'] += x.abs().sum(dim=0)\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2404347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "pruned_model = MyMLP(input_size)\n",
    "\n",
    "# Pass training data through the model to record activations\n",
    "pruned_model.eval()  # Set to eval mode if you don't need to track gradients\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in train_loader:\n",
    "        pruned_model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f5e6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_activations(model, layer_name, amount):\n",
    "    # Get the weight tensor\n",
    "    weight = getattr(model, layer_name).weight.data\n",
    "\n",
    "    # Compute the sum of absolute activations for each output neuron\n",
    "    activation = model.activations[layer_name]\n",
    "\n",
    "    # Calculate the threshold for pruning\n",
    "    threshold = torch.quantile(activation, amount)\n",
    "\n",
    "    # Generate a mask where entries are 1 if above the threshold and 0 otherwise\n",
    "    mask = activation.ge(threshold).float()\n",
    "\n",
    "    # Reshape the mask to match the dimensions of the weight tensor\n",
    "    mask = mask.unsqueeze(1).expand_as(weight)\n",
    "\n",
    "    # Apply the custom pruning mask\n",
    "    prune.custom_from_mask(getattr(model, layer_name), name='weight', mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4bda0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply activation-based pruning\n",
    "prune_activations(pruned_model, 'fc1', amount=0.2)\n",
    "prune_activations(pruned_model, 'fc2', amount=0.2)\n",
    "\n",
    "# Make pruning permanent\n",
    "for layer_name in ['fc1', 'fc2']:\n",
    "    prune.remove(getattr(pruned_model, layer_name), 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd7751c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_pruned_activation.pth\"\n",
    "torch.save(model.state_dict(), pruned_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "628b1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model: 60.96 KB\n",
      "Accuracy on the test set: 13.09%\n",
      "CPU usage during inference: 55.10%\n",
      "Inference time: 0.0079 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_usage, inference_time, _ = measure_cpu_utilization_and_run(compute_metrics_base, pruned_model, x_test_tensor, y_test_tensor, pruned_model_path)\n",
    "\n",
    "print(f'CPU usage during inference: {cpu_usage:.2f}%')\n",
    "print(f'Inference time: {inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "382978e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.15.0-cp311-cp311-macosx_10_12_universal2.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/sandeep/anaconda3/lib/python3.11/site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Users/sandeep/anaconda3/lib/python3.11/site-packages (from onnx) (4.23.4)\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11bb9f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=48, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44036c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "566d4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"/Users/sandeep/Desktop/BUCourses/Project/saved_models/Pytorch/MLP_model.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path, opset_version=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b321287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
