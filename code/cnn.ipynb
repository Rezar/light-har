{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GY3bXc_YQcAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uexqzhBmMrJr"
      },
      "outputs": [],
      "source": [
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "x = genfromtxt('/content/drive/MyDrive/Colab Notebooks/WISDM_x.csv', delimiter=',')\n",
        "y = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/WISDM_y.csv').to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y).reshape(-1,1)\n",
        "\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "y = ohe.fit_transform(y).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i34ss2_4T-NU",
        "outputId": "cca16e3a-c8d1-4c41-ec28-4976f6dcd252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_series(x,y,timestep,overlap):\n",
        "\n",
        "  slide_step = int(timestep*(1-overlap))\n",
        "  data_num = int((len(x)/slide_step)-1)\n",
        "  \n",
        "  dataset = np.ndarray(shape=(data_num,timestep,len(x[0])))\n",
        "  labels = list()\n",
        "\n",
        "  for i in range(data_num):\n",
        "    labels.append(y[slide_step*(i+1)-1])\n",
        "    for j in range(timestep):\n",
        "      dataset[i,j,:] = x[slide_step*i+j,:]\n",
        "\n",
        "  return dataset,np.array(labels)\n"
      ],
      "metadata": {
        "id": "gVkfvWHFWC8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred.append(\"5\")\n",
        "import numpy as np\n",
        "dataset,labels = create_series(x,y,16,0.5)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset,labels,test_size=0.3, random_state=412)\n",
        "# X_test, X_val, y_test, y_val = train_test_split(X_toSplit,y_toSplit,test_size=0.25, random_state=412)"
      ],
      "metadata": {
        "id": "mhk5N1_FUHW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CVbv72WZIWq",
        "outputId": "8d91d06c-c7ae-468f-9554-53e5d64cf3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131070, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Data \n",
        "\n",
        "training_data_count = len(X_train)  # 96092 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 41183 testing series\n",
        "n_steps = len(X_train[0])  # 16 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 3 input parameters per timestep\n",
        "print(n_steps,n_input)\n",
        "\n",
        "# LSTM Neural Network's internal structure\n",
        "# n_hidden = 32 # Hidden layer num of features\n",
        "n_classes = 6 # Total classes (should go up, or should go down)\n",
        "\n",
        "\n",
        "# Training \n",
        "\n",
        "learning_rate = 0.0025\n",
        "lambda_loss_amount = 0.0015\n",
        "training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
        "batch_size = 64\n",
        "display_iter = 30000  # To show test set accuracy during training\n",
        "\n",
        "\n",
        "# Some debugging info\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En2oaW9aS1x-",
        "outputId": "77d896d8-9b3a-45c8-ea77-e3862a4803e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 3\n",
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(39321, 16, 3) (39321, 6) 2.873419266367973 6.908464983446615\n",
            "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv1D(256, 3, input_shape=(n_steps, n_input)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Conv1D(128,3))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Conv1D(64,3))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Conv1D(32,3))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
        "\n",
        "model.fit(X_train,y_train,batch_size=64,epochs=300,validation_split=0.1)\n",
        "\n",
        "model.save('/cnn_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2DGXBiBMRuS_",
        "outputId": "e74b43d1-91d2-4e4b-e711-d3656d768428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1291/1291 [==============================] - 15s 5ms/step - loss: 1.4125 - accuracy: 0.5007 - val_loss: 1.2330 - val_accuracy: 0.5457\n",
            "Epoch 2/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2670 - accuracy: 0.5355 - val_loss: 1.2241 - val_accuracy: 0.5454\n",
            "Epoch 3/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2637 - accuracy: 0.5374 - val_loss: 1.2342 - val_accuracy: 0.5520\n",
            "Epoch 4/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2625 - accuracy: 0.5377 - val_loss: 1.2380 - val_accuracy: 0.5458\n",
            "Epoch 5/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2614 - accuracy: 0.5371 - val_loss: 1.2268 - val_accuracy: 0.5442\n",
            "Epoch 6/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2587 - accuracy: 0.5396 - val_loss: 1.2356 - val_accuracy: 0.5548\n",
            "Epoch 7/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2576 - accuracy: 0.5397 - val_loss: 1.2229 - val_accuracy: 0.5374\n",
            "Epoch 8/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2562 - accuracy: 0.5404 - val_loss: 1.2327 - val_accuracy: 0.5475\n",
            "Epoch 9/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2568 - accuracy: 0.5381 - val_loss: 1.2368 - val_accuracy: 0.5489\n",
            "Epoch 10/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2555 - accuracy: 0.5400 - val_loss: 1.2271 - val_accuracy: 0.5372\n",
            "Epoch 11/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2545 - accuracy: 0.5415 - val_loss: 1.2209 - val_accuracy: 0.5552\n",
            "Epoch 12/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2544 - accuracy: 0.5402 - val_loss: 1.2322 - val_accuracy: 0.5657\n",
            "Epoch 13/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2548 - accuracy: 0.5409 - val_loss: 1.2196 - val_accuracy: 0.5520\n",
            "Epoch 14/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2551 - accuracy: 0.5397 - val_loss: 1.2191 - val_accuracy: 0.5451\n",
            "Epoch 15/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2530 - accuracy: 0.5422 - val_loss: 1.2244 - val_accuracy: 0.5520\n",
            "Epoch 16/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2532 - accuracy: 0.5412 - val_loss: 1.2313 - val_accuracy: 0.5557\n",
            "Epoch 17/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2533 - accuracy: 0.5412 - val_loss: 1.2451 - val_accuracy: 0.5368\n",
            "Epoch 18/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2529 - accuracy: 0.5407 - val_loss: 1.2249 - val_accuracy: 0.5356\n",
            "Epoch 19/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2525 - accuracy: 0.5426 - val_loss: 1.2245 - val_accuracy: 0.5354\n",
            "Epoch 20/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2534 - accuracy: 0.5416 - val_loss: 1.2357 - val_accuracy: 0.5629\n",
            "Epoch 21/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2532 - accuracy: 0.5415 - val_loss: 1.2342 - val_accuracy: 0.5647\n",
            "Epoch 22/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2531 - accuracy: 0.5413 - val_loss: 1.2274 - val_accuracy: 0.5344\n",
            "Epoch 23/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2533 - accuracy: 0.5406 - val_loss: 1.2232 - val_accuracy: 0.5557\n",
            "Epoch 24/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2526 - accuracy: 0.5414 - val_loss: 1.2228 - val_accuracy: 0.5335\n",
            "Epoch 25/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2535 - accuracy: 0.5412 - val_loss: 1.2262 - val_accuracy: 0.5420\n",
            "Epoch 26/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2525 - accuracy: 0.5408 - val_loss: 1.2218 - val_accuracy: 0.5609\n",
            "Epoch 27/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2526 - accuracy: 0.5420 - val_loss: 1.2371 - val_accuracy: 0.5489\n",
            "Epoch 28/300\n",
            "1291/1291 [==============================] - 8s 6ms/step - loss: 1.2522 - accuracy: 0.5406 - val_loss: 1.2228 - val_accuracy: 0.5541\n",
            "Epoch 29/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2534 - accuracy: 0.5413 - val_loss: 1.2231 - val_accuracy: 0.5494\n",
            "Epoch 30/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2523 - accuracy: 0.5409 - val_loss: 1.2232 - val_accuracy: 0.5329\n",
            "Epoch 31/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2526 - accuracy: 0.5407 - val_loss: 1.2294 - val_accuracy: 0.5597\n",
            "Epoch 32/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2518 - accuracy: 0.5424 - val_loss: 1.2276 - val_accuracy: 0.5319\n",
            "Epoch 33/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5419 - val_loss: 1.2225 - val_accuracy: 0.5552\n",
            "Epoch 34/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2529 - accuracy: 0.5409 - val_loss: 1.2344 - val_accuracy: 0.5603\n",
            "Epoch 35/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2528 - accuracy: 0.5397 - val_loss: 1.2242 - val_accuracy: 0.5544\n",
            "Epoch 36/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2518 - accuracy: 0.5416 - val_loss: 1.2229 - val_accuracy: 0.5530\n",
            "Epoch 37/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2526 - accuracy: 0.5406 - val_loss: 1.2263 - val_accuracy: 0.5589\n",
            "Epoch 38/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2509 - accuracy: 0.5428 - val_loss: 1.2277 - val_accuracy: 0.5507\n",
            "Epoch 39/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2523 - accuracy: 0.5423 - val_loss: 1.2216 - val_accuracy: 0.5588\n",
            "Epoch 40/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2520 - accuracy: 0.5415 - val_loss: 1.2221 - val_accuracy: 0.5626\n",
            "Epoch 41/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2530 - accuracy: 0.5408 - val_loss: 1.2194 - val_accuracy: 0.5587\n",
            "Epoch 42/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2524 - accuracy: 0.5398 - val_loss: 1.2219 - val_accuracy: 0.5513\n",
            "Epoch 43/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2521 - accuracy: 0.5422 - val_loss: 1.2182 - val_accuracy: 0.5477\n",
            "Epoch 44/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2525 - accuracy: 0.5419 - val_loss: 1.2303 - val_accuracy: 0.5639\n",
            "Epoch 45/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2519 - accuracy: 0.5423 - val_loss: 1.2197 - val_accuracy: 0.5519\n",
            "Epoch 46/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2515 - accuracy: 0.5439 - val_loss: 1.2241 - val_accuracy: 0.5396\n",
            "Epoch 47/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2530 - accuracy: 0.5413 - val_loss: 1.2155 - val_accuracy: 0.5520\n",
            "Epoch 48/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2514 - accuracy: 0.5416 - val_loss: 1.2519 - val_accuracy: 0.5631\n",
            "Epoch 49/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2514 - accuracy: 0.5414 - val_loss: 1.2214 - val_accuracy: 0.5440\n",
            "Epoch 50/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2528 - accuracy: 0.5407 - val_loss: 1.2248 - val_accuracy: 0.5567\n",
            "Epoch 51/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2509 - accuracy: 0.5409 - val_loss: 1.2216 - val_accuracy: 0.5578\n",
            "Epoch 52/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2514 - accuracy: 0.5417 - val_loss: 1.2191 - val_accuracy: 0.5495\n",
            "Epoch 53/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2525 - accuracy: 0.5416 - val_loss: 1.2264 - val_accuracy: 0.5513\n",
            "Epoch 54/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2512 - accuracy: 0.5416 - val_loss: 1.2306 - val_accuracy: 0.5441\n",
            "Epoch 55/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2524 - accuracy: 0.5410 - val_loss: 1.2214 - val_accuracy: 0.5595\n",
            "Epoch 56/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2537 - accuracy: 0.5412 - val_loss: 1.2210 - val_accuracy: 0.5368\n",
            "Epoch 57/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2521 - accuracy: 0.5418 - val_loss: 1.2172 - val_accuracy: 0.5458\n",
            "Epoch 58/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2536 - accuracy: 0.5418 - val_loss: 1.2211 - val_accuracy: 0.5506\n",
            "Epoch 59/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2517 - accuracy: 0.5410 - val_loss: 1.2234 - val_accuracy: 0.5587\n",
            "Epoch 60/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2508 - accuracy: 0.5426 - val_loss: 1.2207 - val_accuracy: 0.5496\n",
            "Epoch 61/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2522 - accuracy: 0.5413 - val_loss: 1.2269 - val_accuracy: 0.5443\n",
            "Epoch 62/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2521 - accuracy: 0.5404 - val_loss: 1.2305 - val_accuracy: 0.5571\n",
            "Epoch 63/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2518 - accuracy: 0.5406 - val_loss: 1.2299 - val_accuracy: 0.5647\n",
            "Epoch 64/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5423 - val_loss: 1.2300 - val_accuracy: 0.5569\n",
            "Epoch 65/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2512 - accuracy: 0.5400 - val_loss: 1.2207 - val_accuracy: 0.5354\n",
            "Epoch 66/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2519 - accuracy: 0.5412 - val_loss: 1.2173 - val_accuracy: 0.5449\n",
            "Epoch 67/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5413 - val_loss: 1.2391 - val_accuracy: 0.5464\n",
            "Epoch 68/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2528 - accuracy: 0.5412 - val_loss: 1.2322 - val_accuracy: 0.5579\n",
            "Epoch 69/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5409 - val_loss: 1.2208 - val_accuracy: 0.5622\n",
            "Epoch 70/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2525 - accuracy: 0.5419 - val_loss: 1.2234 - val_accuracy: 0.5450\n",
            "Epoch 71/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2524 - accuracy: 0.5415 - val_loss: 1.2238 - val_accuracy: 0.5479\n",
            "Epoch 72/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2518 - accuracy: 0.5416 - val_loss: 1.2327 - val_accuracy: 0.5590\n",
            "Epoch 73/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2513 - accuracy: 0.5423 - val_loss: 1.2244 - val_accuracy: 0.5439\n",
            "Epoch 74/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2523 - accuracy: 0.5413 - val_loss: 1.2282 - val_accuracy: 0.5556\n",
            "Epoch 75/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2514 - accuracy: 0.5418 - val_loss: 1.2197 - val_accuracy: 0.5650\n",
            "Epoch 76/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2521 - accuracy: 0.5423 - val_loss: 1.2216 - val_accuracy: 0.5525\n",
            "Epoch 77/300\n",
            "1291/1291 [==============================] - 8s 6ms/step - loss: 1.2522 - accuracy: 0.5421 - val_loss: 1.2231 - val_accuracy: 0.5526\n",
            "Epoch 78/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5425 - val_loss: 1.2325 - val_accuracy: 0.5652\n",
            "Epoch 79/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2509 - accuracy: 0.5417 - val_loss: 1.2249 - val_accuracy: 0.5464\n",
            "Epoch 80/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2527 - accuracy: 0.5418 - val_loss: 1.2235 - val_accuracy: 0.5488\n",
            "Epoch 81/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2527 - accuracy: 0.5420 - val_loss: 1.2379 - val_accuracy: 0.5491\n",
            "Epoch 82/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2526 - accuracy: 0.5411 - val_loss: 1.2230 - val_accuracy: 0.5491\n",
            "Epoch 83/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2523 - accuracy: 0.5409 - val_loss: 1.2205 - val_accuracy: 0.5508\n",
            "Epoch 84/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2519 - accuracy: 0.5426 - val_loss: 1.2328 - val_accuracy: 0.5482\n",
            "Epoch 85/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2515 - accuracy: 0.5423 - val_loss: 1.2196 - val_accuracy: 0.5499\n",
            "Epoch 86/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2517 - accuracy: 0.5424 - val_loss: 1.2205 - val_accuracy: 0.5567\n",
            "Epoch 87/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2515 - accuracy: 0.5420 - val_loss: 1.2243 - val_accuracy: 0.5646\n",
            "Epoch 88/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2536 - accuracy: 0.5419 - val_loss: 1.2279 - val_accuracy: 0.5612\n",
            "Epoch 89/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2519 - accuracy: 0.5417 - val_loss: 1.2201 - val_accuracy: 0.5535\n",
            "Epoch 90/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2520 - accuracy: 0.5421 - val_loss: 1.2168 - val_accuracy: 0.5474\n",
            "Epoch 91/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2525 - accuracy: 0.5417 - val_loss: 1.2311 - val_accuracy: 0.5331\n",
            "Epoch 92/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2513 - accuracy: 0.5422 - val_loss: 1.2366 - val_accuracy: 0.5474\n",
            "Epoch 93/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2518 - accuracy: 0.5412 - val_loss: 1.2256 - val_accuracy: 0.5561\n",
            "Epoch 94/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2521 - accuracy: 0.5417 - val_loss: 1.2302 - val_accuracy: 0.5623\n",
            "Epoch 95/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2514 - accuracy: 0.5434 - val_loss: 1.2289 - val_accuracy: 0.5359\n",
            "Epoch 96/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2526 - accuracy: 0.5420 - val_loss: 1.2189 - val_accuracy: 0.5554\n",
            "Epoch 97/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2507 - accuracy: 0.5428 - val_loss: 1.2186 - val_accuracy: 0.5431\n",
            "Epoch 98/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5416 - val_loss: 1.2182 - val_accuracy: 0.5401\n",
            "Epoch 99/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2507 - accuracy: 0.5409 - val_loss: 1.2183 - val_accuracy: 0.5470\n",
            "Epoch 100/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2506 - accuracy: 0.5422 - val_loss: 1.2243 - val_accuracy: 0.5445\n",
            "Epoch 101/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2531 - accuracy: 0.5425 - val_loss: 1.2202 - val_accuracy: 0.5515\n",
            "Epoch 102/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2525 - accuracy: 0.5424 - val_loss: 1.2220 - val_accuracy: 0.5483\n",
            "Epoch 103/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2520 - accuracy: 0.5423 - val_loss: 1.2201 - val_accuracy: 0.5508\n",
            "Epoch 104/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2520 - accuracy: 0.5413 - val_loss: 1.2204 - val_accuracy: 0.5540\n",
            "Epoch 105/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2522 - accuracy: 0.5427 - val_loss: 1.2201 - val_accuracy: 0.5581\n",
            "Epoch 106/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2504 - accuracy: 0.5424 - val_loss: 1.2248 - val_accuracy: 0.5661\n",
            "Epoch 107/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2521 - accuracy: 0.5409 - val_loss: 1.2185 - val_accuracy: 0.5440\n",
            "Epoch 108/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2519 - accuracy: 0.5410 - val_loss: 1.2435 - val_accuracy: 0.5472\n",
            "Epoch 109/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2527 - accuracy: 0.5418 - val_loss: 1.2182 - val_accuracy: 0.5573\n",
            "Epoch 110/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2513 - accuracy: 0.5421 - val_loss: 1.2251 - val_accuracy: 0.5504\n",
            "Epoch 111/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5414 - val_loss: 1.2255 - val_accuracy: 0.5397\n",
            "Epoch 112/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2531 - accuracy: 0.5417 - val_loss: 1.2222 - val_accuracy: 0.5442\n",
            "Epoch 113/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2518 - accuracy: 0.5427 - val_loss: 1.2239 - val_accuracy: 0.5444\n",
            "Epoch 114/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2517 - accuracy: 0.5428 - val_loss: 1.2168 - val_accuracy: 0.5539\n",
            "Epoch 115/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2516 - accuracy: 0.5418 - val_loss: 1.2203 - val_accuracy: 0.5571\n",
            "Epoch 116/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2522 - accuracy: 0.5426 - val_loss: 1.2313 - val_accuracy: 0.5519\n",
            "Epoch 117/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2512 - accuracy: 0.5419 - val_loss: 1.2181 - val_accuracy: 0.5479\n",
            "Epoch 118/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2524 - accuracy: 0.5429 - val_loss: 1.2239 - val_accuracy: 0.5336\n",
            "Epoch 119/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5422 - val_loss: 1.2188 - val_accuracy: 0.5527\n",
            "Epoch 120/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2522 - accuracy: 0.5418 - val_loss: 1.2243 - val_accuracy: 0.5427\n",
            "Epoch 121/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5411 - val_loss: 1.2183 - val_accuracy: 0.5510\n",
            "Epoch 122/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2521 - accuracy: 0.5416 - val_loss: 1.2200 - val_accuracy: 0.5580\n",
            "Epoch 123/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2526 - accuracy: 0.5419 - val_loss: 1.2351 - val_accuracy: 0.5538\n",
            "Epoch 124/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2522 - accuracy: 0.5411 - val_loss: 1.2187 - val_accuracy: 0.5484\n",
            "Epoch 125/300\n",
            "1291/1291 [==============================] - 7s 5ms/step - loss: 1.2524 - accuracy: 0.5415 - val_loss: 1.2311 - val_accuracy: 0.5369\n",
            "Epoch 126/300\n",
            "1291/1291 [==============================] - 7s 5ms/step - loss: 1.2521 - accuracy: 0.5418 - val_loss: 1.2207 - val_accuracy: 0.5611\n",
            "Epoch 127/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2515 - accuracy: 0.5416 - val_loss: 1.2336 - val_accuracy: 0.5468\n",
            "Epoch 128/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2519 - accuracy: 0.5419 - val_loss: 1.2311 - val_accuracy: 0.5599\n",
            "Epoch 129/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2521 - accuracy: 0.5415 - val_loss: 1.2218 - val_accuracy: 0.5508\n",
            "Epoch 130/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2517 - accuracy: 0.5424 - val_loss: 1.2295 - val_accuracy: 0.5331\n",
            "Epoch 131/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2509 - accuracy: 0.5406 - val_loss: 1.2247 - val_accuracy: 0.5506\n",
            "Epoch 132/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2516 - accuracy: 0.5405 - val_loss: 1.2259 - val_accuracy: 0.5462\n",
            "Epoch 133/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2514 - accuracy: 0.5415 - val_loss: 1.2337 - val_accuracy: 0.5592\n",
            "Epoch 134/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2520 - accuracy: 0.5420 - val_loss: 1.2193 - val_accuracy: 0.5454\n",
            "Epoch 135/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2517 - accuracy: 0.5420 - val_loss: 1.2286 - val_accuracy: 0.5482\n",
            "Epoch 136/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2516 - accuracy: 0.5416 - val_loss: 1.2197 - val_accuracy: 0.5505\n",
            "Epoch 137/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2513 - accuracy: 0.5424 - val_loss: 1.2223 - val_accuracy: 0.5507\n",
            "Epoch 138/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2520 - accuracy: 0.5416 - val_loss: 1.2256 - val_accuracy: 0.5625\n",
            "Epoch 139/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2515 - accuracy: 0.5418 - val_loss: 1.2416 - val_accuracy: 0.5560\n",
            "Epoch 140/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2517 - accuracy: 0.5427 - val_loss: 1.2242 - val_accuracy: 0.5547\n",
            "Epoch 141/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2510 - accuracy: 0.5432 - val_loss: 1.2266 - val_accuracy: 0.5588\n",
            "Epoch 142/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2511 - accuracy: 0.5426 - val_loss: 1.2205 - val_accuracy: 0.5365\n",
            "Epoch 143/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2520 - accuracy: 0.5418 - val_loss: 1.2188 - val_accuracy: 0.5527\n",
            "Epoch 144/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2524 - accuracy: 0.5411 - val_loss: 1.2252 - val_accuracy: 0.5351\n",
            "Epoch 145/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2515 - accuracy: 0.5416 - val_loss: 1.2199 - val_accuracy: 0.5577\n",
            "Epoch 146/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2516 - accuracy: 0.5424 - val_loss: 1.2210 - val_accuracy: 0.5406\n",
            "Epoch 147/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2517 - accuracy: 0.5417 - val_loss: 1.2305 - val_accuracy: 0.5387\n",
            "Epoch 148/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2515 - accuracy: 0.5417 - val_loss: 1.2227 - val_accuracy: 0.5411\n",
            "Epoch 149/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2519 - accuracy: 0.5437 - val_loss: 1.2350 - val_accuracy: 0.5455\n",
            "Epoch 150/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2518 - accuracy: 0.5423 - val_loss: 1.2248 - val_accuracy: 0.5566\n",
            "Epoch 151/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2514 - accuracy: 0.5426 - val_loss: 1.2217 - val_accuracy: 0.5489\n",
            "Epoch 152/300\n",
            "1291/1291 [==============================] - 6s 4ms/step - loss: 1.2509 - accuracy: 0.5427 - val_loss: 1.2204 - val_accuracy: 0.5460\n",
            "Epoch 153/300\n",
            "1291/1291 [==============================] - 6s 5ms/step - loss: 1.2520 - accuracy: 0.5403 - val_loss: 1.2198 - val_accuracy: 0.5460\n",
            "Epoch 154/300\n",
            "1283/1291 [============================>.] - ETA: 0s - loss: 1.2517 - accuracy: 0.5422"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f11533122d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cnn_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1454\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1457\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1756\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1757\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}