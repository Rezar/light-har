{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"nncf>=2.5.0\"\n",
    "%pip install -q \"openvino>=2023.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n",
      "Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, openvino\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import the s4 model path\n",
    "import sys\n",
    "sys.path.append('/Users/poomchan/Developer/s4')\n",
    "\n",
    "from models.s4.s4 import S4Block as S4\n",
    "from models.s4.s4d import S4D, S4DKernel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import nncf\n",
    "from nncf.parameters import ModelType\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = \"/Users/poomchan/Developer/light-har\"\n",
    "DATA_DIR = PROJECT_DIR + \"/data\"\n",
    "MODEL_DIR = PROJECT_DIR + \"/code/s4/models\"\n",
    "\n",
    "# Torch device configuration\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "    if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 16, 3])\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "x = genfromtxt(PROJECT_DIR+'/data/WISDM_x.csv', delimiter=',')\n",
    "y_df = pd.read_csv(PROJECT_DIR+'/data/WISDM_y.csv')\n",
    "y = y_df.values.flatten()  # Flatten if y is 2D\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Function to create time series dataset\n",
    "def create_series(x, y, timestep, overlap):\n",
    "    slide_step = int(timestep * (1 - overlap))\n",
    "    data_num = int((len(x) / slide_step) - 1)\n",
    "    dataset = np.ndarray(shape=(data_num, timestep, x.shape[1]))\n",
    "    labels = []\n",
    "\n",
    "    for i in range(data_num):\n",
    "        labels.append(y[slide_step * (i + 1) - 1])\n",
    "        for j in range(timestep):\n",
    "            dataset[i, j, :] = x[slide_step * i + j, :]\n",
    "\n",
    "    return dataset, np.array(labels)\n",
    "\n",
    "# Create time series\n",
    "seq_length = 16\n",
    "overlap = 0.5\n",
    "X_series, y_series = create_series(x, y_encoded, seq_length, overlap)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_series, y_series, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "x_cal_tensor = x_train_tensor[:300]\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "y_cal_tensor = y_train_tensor[:300]\n",
    "\n",
    "print(x_cal_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create a test Dataloader\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create a calibration Dataloader\n",
    "calibration_loader = DataLoader(TensorDataset(x_cal_tensor, y_cal_tensor), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output,\n",
    "        d_model=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.2,\n",
    "        lr=0.001,\n",
    "        dropout_fn=nn.Dropout,\n",
    "        prenorm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4D(d_model, dropout=dropout, transposed=True, lr=lr)\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(dropout_fn(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = S4Model(\n",
    "    d_input=3, # num of feature\n",
    "    d_output=6, # 6 classes\n",
    "    d_model=16,\n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    lr=0.001,\n",
    "    dropout_fn=nn.Dropout,\n",
    "    prenorm=False,\n",
    ")\n",
    "\n",
    "state_path = f\"{MODEL_DIR}/s4-d16.pt\"\n",
    "state_dict = torch.load(state_path, map_location='mps')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 92.40 %\n",
      "Size of the model: 55.87 KB\n",
      "Total inference time: 3.06 seconds\n",
      "CPU Utilization: 51.80 %\n"
     ]
    }
   ],
   "source": [
    "utils.run_measurements(model, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNCF Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 08:21:46.704797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:nncf:NNCF provides best results with torch==2.1.2, while current torch version is 2.1.0. If you encounter issues, consider switching to torch==2.1.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c1d331add44846951b2a8a28aa57db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Compiling and loading torch extension: quantized_functions_cpu...\n",
      "INFO:nncf:Finished loading torch extension: quantized_functions_cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0385ffd6f9444d1799157b9f1d3465e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The calibration dataset is a small, no label, representative dataset\n",
    "# (~100-500 samples) that is used to estimate the range, i.e. (min, max) of all\n",
    "# floating point activation tensors in the model, to initialize the quantization\n",
    "# parameters.\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    features, label = data_item\n",
    "    return features\n",
    "\n",
    "calibration_dataset = nncf.Dataset(calibration_loader, transform_fn)\n",
    "\n",
    "quantized_model = nncf.quantize(model, calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S4Model(\n",
       "  (encoder): NNCFLinear(\n",
       "    in_features=3, out_features=16, bias=True\n",
       "    (pre_ops): ModuleDict(\n",
       "      (0): UpdateWeight(\n",
       "        (op): SymmetricQuantizer(bit=8, ch=True)\n",
       "      )\n",
       "    )\n",
       "    (post_ops): ModuleDict()\n",
       "  )\n",
       "  (s4_layers): ModuleList(\n",
       "    (0-3): 4 x S4D(\n",
       "      (kernel): S4DKernel()\n",
       "      (activation): GELU(approximate='none')\n",
       "      (dropout): DropoutNd()\n",
       "      (output_linear): Sequential(\n",
       "        (0): NNCFConv1d(\n",
       "          16, 32, kernel_size=(1,), stride=(1,)\n",
       "          (pre_ops): ModuleDict(\n",
       "            (0): UpdateWeight(\n",
       "              (op): SymmetricQuantizer(bit=8, ch=True)\n",
       "            )\n",
       "          )\n",
       "          (post_ops): ModuleDict()\n",
       "        )\n",
       "        (1): GLU(dim=-2)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norms): ModuleList(\n",
       "    (0-3): 4 x NNCFLayerNorm(\n",
       "      (16,), eps=1e-05, elementwise_affine=True\n",
       "      (pre_ops): ModuleDict(\n",
       "        (0): UpdateWeight(\n",
       "          (op): SymmetricQuantizer(bit=8, ch=False)\n",
       "        )\n",
       "      )\n",
       "      (post_ops): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (dropouts): ModuleList(\n",
       "    (0-3): 4 x Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): NNCFLinear(\n",
       "    in_features=16, out_features=6, bias=True\n",
       "    (pre_ops): ModuleDict(\n",
       "      (0): UpdateWeight(\n",
       "        (op): SymmetricQuantizer(bit=8, ch=True)\n",
       "      )\n",
       "    )\n",
       "    (post_ops): ModuleDict()\n",
       "  )\n",
       "  (_nncf): NNCFNetworkInterface(\n",
       "    (external_quantizers): ModuleDict(\n",
       "      (/nncf_model_input_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/NNCFLinear[encoder]/linear_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[0]/__mul___0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[0]/GELU[activation]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[0]/Sequential[output_linear]/GLU[1]/glu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/__add___0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[norms]/NNCFLayerNorm[0]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[1]/__mul___0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[1]/GELU[activation]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[1]/Sequential[output_linear]/GLU[1]/glu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/__add___1|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[norms]/NNCFLayerNorm[1]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[2]/__mul___0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[2]/GELU[activation]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[2]/Sequential[output_linear]/GLU[1]/glu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/__add___2|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[norms]/NNCFLayerNorm[2]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[3]/__mul___0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[3]/GELU[activation]/gelu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[s4_layers]/S4D[3]/Sequential[output_linear]/GLU[1]/glu_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/__add___3|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/ModuleList[norms]/NNCFLayerNorm[3]/layer_norm_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "      (S4Model/mean_0|OUTPUT): SymmetricQuantizer(bit=8, ch=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 92.47 %\n",
      "Size of the model: 105.96 KB\n",
      "Total inference time: 14.37 seconds\n",
      "CPU Utilization: 43.50 %\n"
     ]
    }
   ],
   "source": [
    "utils.run_measurements(quantized_model, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenVino Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "ename": "OpConversionFailure",
     "evalue": "Check 'is_conversion_successful' failed at src/frontends/pytorch/src/frontend.cpp:143:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- ov::align_types with a message:\nThis is internal operation for type alignment and should be removed at normalization step. It can't be removed if types can't be resolved.\n-- prim::Constant with a message:\nNone constant cannot be converted to OpenVINO opset and should be removed by consuming operation.\nSummary:\n-- No conversion rule found for operations: aten::fft_irfft, aten::fft_rfft, aten::real, aten::view_as_complex\n-- Conversion is failed for: ov::align_types, prim::Constant\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpConversionFailure\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mov\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ov_quantized_model \u001b[38;5;241m=\u001b[39m ov\u001b[38;5;241m.\u001b[39mconvert_model(quantized_model\u001b[38;5;241m.\u001b[39mcpu(), example_input\u001b[38;5;241m=\u001b[39mdummy_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/openvino/tools/ovc/convert.py:100\u001b[0m, in \u001b[0;36mconvert_model\u001b[0;34m(input_model, input, output, example_input, extension, verbose, share_weights)\u001b[0m\n\u001b[1;32m     98\u001b[0m logger_state \u001b[38;5;241m=\u001b[39m get_logger_state()\n\u001b[1;32m     99\u001b[0m cli_parser \u001b[38;5;241m=\u001b[39m get_all_cli_parser()\n\u001b[0;32m--> 100\u001b[0m ov_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcli_parser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m restore_logger_state(logger_state)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/openvino/tools/ovc/convert_impl.py:535\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(cli_parser, args, python_api_used)\u001b[0m\n\u001b[1;32m    533\u001b[0m send_conversion_result(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfail\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m python_api_used:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, argv\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/openvino/tools/ovc/convert_impl.py:477\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(cli_parser, args, python_api_used)\u001b[0m\n\u001b[1;32m    473\u001b[0m argv\u001b[38;5;241m.\u001b[39mis_python_api_used \u001b[38;5;241m=\u001b[39m python_api_used\n\u001b[1;32m    475\u001b[0m argv\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m=\u001b[39m model_framework\n\u001b[0;32m--> 477\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversion_parameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_default_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inp_model_is_object \u001b[38;5;129;01mand\u001b[39;00m model_framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaddle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paddle_runtime_converter:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/openvino/tools/ovc/convert_impl.py:228\u001b[0m, in \u001b[0;36mdriver\u001b[0;34m(argv, non_default_params)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Log dictionary with non-default cli parameters where complex classes are excluded.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;28mstr\u001b[39m(non_default_params))\n\u001b[0;32m--> 228\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m moc_emit_ir(\u001b[43mprepare_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m, argv)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/openvino/tools/ovc/convert_impl.py:177\u001b[0m, in \u001b[0;36mprepare_ir\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m extension \u001b[38;5;129;01min\u001b[39;00m argv\u001b[38;5;241m.\u001b[39mextension:\n\u001b[1;32m    176\u001b[0m             moc_front_end\u001b[38;5;241m.\u001b[39madd_extension(extension)\n\u001b[0;32m--> 177\u001b[0m     ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mmoc_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoc_front_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m argv\u001b[38;5;241m.\u001b[39minput_model:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/openvino/tools/ovc/moc_frontend/pipeline.py:244\u001b[0m, in \u001b[0;36mmoc_pipeline\u001b[0;34m(argv, moc_front_end)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape_to_array\u001b[39m(shape: PartialShape):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [shape\u001b[38;5;241m.\u001b[39mget_dimension(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(shape\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;241m.\u001b[39mget_length())]\n\u001b[0;32m--> 244\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mmoc_front_end\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/openvino/frontend/frontend.py:18\u001b[0m, in \u001b[0;36mFrontEnd.convert\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: Union[Model, InputModel]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:\n\u001b[0;32m---> 18\u001b[0m     converted_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, InputModel):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Model(converted_model)\n",
      "\u001b[0;31mOpConversionFailure\u001b[0m: Check 'is_conversion_successful' failed at src/frontends/pytorch/src/frontend.cpp:143:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- ov::align_types with a message:\nThis is internal operation for type alignment and should be removed at normalization step. It can't be removed if types can't be resolved.\n-- prim::Constant with a message:\nNone constant cannot be converted to OpenVINO opset and should be removed by consuming operation.\nSummary:\n-- No conversion rule found for operations: aten::fft_irfft, aten::fft_rfft, aten::real, aten::view_as_complex\n-- Conversion is failed for: ov::align_types, prim::Constant\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 16, 3)\n",
    "ov_model = ov.convert_model(model.cpu(), example_input=dummy_input)\n",
    "ov_quantized_model = ov.convert_model(quantized_model.cpu(), example_input=dummy_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
